# -*- coding: UTF-8 -*-
"""
è‡ªåŠ¨é©¾é©¶ç›®æ ‡æ£€æµ‹ç³»ç»Ÿ - ä¸»çª—å£ç±»
"""
import sys
import os
import time
import numpy as np
from PyQt5.QtWidgets import QMainWindow, QFileDialog, QInputDialog
from PyQt5.QtCore import Qt, QThread, pyqtSignal
from aiui import AutonomousDrivingUISetup
from aicamera import CameraVideoHandler
import torch
import cv2
from PyQt5.QtGui import QPixmap, QImage


class InferenceThread(QThread):
    """åå°æ¨ç†çº¿ç¨‹ï¼šæ¥æ”¶æœ€æ–°ä¸€å¸§ï¼ŒæŒ‰è®¾ç½®èŠ‚æµåè¿›è¡ŒYOLOæ¨ç†å¹¶å‘å›å åŠ åçš„QImageï¼ˆå…¼å®¹v5/v8ï¼‰"""

    frame_ready = pyqtSignal(QImage)
    meta_ready = pyqtSignal(str)

    def __init__(self, model, infer_size: int, frame_skip: int, interval_ms: int, model_type: str = 'v5', conf: float = 0.5, iou: float = 0.45):
        super().__init__()
        self.model = model
        self.infer_size = infer_size
        self.frame_skip = max(1, int(frame_skip))
        self.interval_ms = max(0, int(interval_ms))
        self.model_type = 'v8' if (str(model_type).lower().find('v8') != -1 or str(model_type).lower().find('8') != -1) else 'v5'
        self.confidence = float(conf)
        self.iou = float(iou)
        self._running = True
        self._busy = False
        self._last_infer_ms = 0
        self._frame_index = 0
        self._latest_frame = None  # BGR np.ndarray
        # v1 æ¨ç†ä¸Šä¸‹æ–‡ï¼ˆé¢„å¤„ç†/è§£ç æ‰€éœ€å‚æ•°ä¸å‡½æ•°ï¼‰
        self.v1_ctx = None

    def update_params(self, infer_size: int = None, frame_skip: int = None, interval_ms: int = None, conf: float = None, iou: float = None, model_type: str = None):
        if infer_size is not None:
            self.infer_size = int(infer_size)
        if frame_skip is not None:
            self.frame_skip = max(1, int(frame_skip))
        if interval_ms is not None:
            self.interval_ms = max(0, int(interval_ms))
        if conf is not None:
            self.confidence = float(conf)
        if iou is not None:
            self.iou = float(iou)
        if model_type is not None:
            self.model_type = 'v8' if (str(model_type).lower().find('v8') != -1 or str(model_type).lower().find('8') != -1) else 'v5'

    def set_v1_context(self, input_size: int, grid_size: int, num_classes: int, class_names, decode_func, device: str):
        """ä¸º YOLOv1 è®¾ç½®é¢„å¤„ç†ä¸åå¤„ç†ä¸Šä¸‹æ–‡ã€‚"""
        self.v1_ctx = {
            'input_size': int(input_size),
            'grid_size': int(grid_size),
            'num_classes': int(num_classes),
            'class_names': class_names,
            'decode_func': decode_func,
            'device': device,
        }

    def submit_frame(self, frame_bgr):
        # ç¡®ä¿æŒæœ‰ç‹¬ç«‹å‰¯æœ¬ï¼Œé¿å…å¼•ç”¨ä¸´æ—¶ç¼“å†²å¯¼è‡´å´©æºƒ
        try:
            self._latest_frame = frame_bgr.copy()
        except Exception:
            self._latest_frame = frame_bgr

    def stop(self):
        self._running = False

    def run(self):
        while self._running:
            try:
                frame = self._latest_frame
                if frame is None:
                    self.msleep(5)
                    continue

                # å¸§é‡‡æ ·
                self._frame_index += 1
                if (self._frame_index % self.frame_skip) != 0:
                    self.msleep(1)
                    continue

                now_ms = int(time.time() * 1000)
                if self._busy or (now_ms - self._last_infer_ms < self.interval_ms):
                    self.msleep(1)
                    continue

                self._busy = True
                try:
                    with torch.no_grad():
                        if self.model_type == 'v8':
                            # YOLOv8 æ¨ç†
                            results = self.model(frame, imgsz=int(self.infer_size), conf=float(self.confidence), iou=float(self.iou))
                            annotated_bgr = results[0].plot()
                            names = getattr(results[0], 'names', getattr(self.model, 'names', {}))
                            # å…ƒä¿¡æ¯
                            try:
                                boxes = getattr(results[0], 'boxes', None)
                                if boxes is not None and hasattr(boxes, 'xyxy') and boxes.xyxy is not None and boxes.xyxy.shape[0] > 0:
                                    ts_ms = int(time.time() * 1000)
                                    ts = time.strftime("%H:%M:%S", time.localtime(ts_ms / 1000)) + f".{ts_ms % 1000:03d}"
                                    lines = []
                                    count = boxes.xyxy.shape[0]
                                    for i in range(count):
                                        x1, y1, x2, y2 = [int(v) for v in boxes.xyxy[i].detach().cpu().numpy().tolist()]
                                        conf = float(boxes.conf[i].detach().cpu().numpy().tolist())
                                        cls = int(boxes.cls[i].detach().cpu().numpy().tolist())
                                        label = names[cls] if isinstance(names, (list, dict)) else (names[cls] if isinstance(names, list) else f'class{cls}')
                                        lines.append(f"{i + 1}. æ—¶é—´ {ts} | ç±»åˆ« {label} | ä½ç½® ({x1},{y1},{x2},{y2}) | ç½®ä¿¡åº¦ {conf:.2f}")
                                    self.meta_ready.emit("\n".join(lines))
                            except Exception:
                                pass
                        elif self.model_type in ('v5', 'v3'):
                            # YOLOv5/YOLOv3 æ¨ç†ï¼ˆç›¸åŒç»“æœæ¥å£ï¼‰
                            results = self.model(frame, size=int(self.infer_size))
                            annotated_bgr = results.render()[0]
                            names = getattr(results, 'names', getattr(self.model, 'names', {}))
                            # å…ƒä¿¡æ¯
                            try:
                                pred = results.xyxy[0]
                                if pred is not None and pred.shape[0] > 0:
                                    ts_ms = int(time.time() * 1000)
                                    ts = time.strftime("%H:%M:%S", time.localtime(ts_ms / 1000)) + f".{ts_ms % 1000:03d}"
                                    lines = []
                                    for i in range(int(pred.shape[0])):
                                        x1, y1, x2, y2 = [int(v) for v in pred[i, :4].detach().cpu().numpy().tolist()]
                                        conf = float(pred[i, 4])
                                        cls = int(pred[i, 5])
                                        label = names[cls] if isinstance(names, (list, dict)) else f'class{cls}'
                                        lines.append(f"{i + 1}. æ—¶é—´ {ts} | ç±»åˆ« {label} | ä½ç½® ({x1},{y1},{x2},{y2}) | ç½®ä¿¡åº¦ {conf:.2f}")
                                    self.meta_ready.emit("\n".join(lines))
                            except Exception:
                                pass
                        elif self.model_type == 'v1' and self.v1_ctx is not None:
                            # YOLOv1 æ¨ç†
                            try:
                                ctx = self.v1_ctx
                                input_size = ctx['input_size']
                                # é¢„å¤„ç†ï¼šRGBã€æ–¹å½¢å¡«å……ã€ç¼©æ”¾ã€å½’ä¸€åŒ–
                                rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                                h0, w0 = rgb.shape[:2]
                                max_edge = max(h0, w0)
                                top = (max_edge - h0) // 2
                                bottom = max_edge - h0 - top
                                left = (max_edge - w0) // 2
                                right = max_edge - w0 - left
                                padded = cv2.copyMakeBorder(rgb, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])
                                resized = cv2.resize(padded, (input_size, input_size))
                                tensor = torch.from_numpy(resized).float().permute(2, 0, 1) / 255.0
                                mean = torch.tensor([0.408, 0.448, 0.471]).view(3, 1, 1)
                                std = torch.tensor([0.242, 0.239, 0.234]).view(3, 1, 1)
                                tensor = (tensor - mean) / std
                                tensor = tensor.unsqueeze(0).to(ctx['device'])

                                # å‰å‘
                                preds = self.model(tensor)
                                decoded_list = ctx['decode_func'](
                                    preds.detach().cpu(),
                                    conf_threshold=float(self.confidence),
                                    nms_threshold=float(self.iou),
                                    input_size=input_size,
                                    grid_size=int(ctx['grid_size']),
                                    num_classes=int(ctx['num_classes'])
                                )
                                det = decoded_list[0]
                                boxes = det.get('boxes', None)
                                scores = det.get('scores', None)
                                cls_ids = det.get('class_ids', None)

                                # åæ ‡åå˜æ¢å›åŸå›¾
                                if boxes is not None and len(boxes) > 0:
                                    scale = max_edge / input_size
                                    boxes = boxes.copy() * scale
                                    boxes[:, [0, 2]] -= left
                                    boxes[:, [1, 3]] -= top
                                    boxes[:, [0, 2]] = np.clip(boxes[:, [0, 2]], 0, w0)
                                    boxes[:, [1, 3]] = np.clip(boxes[:, [1, 3]], 0, h0)

                                # å åŠ å¯è§†åŒ–
                                annotated_bgr = frame.copy()
                                names = ctx['class_names'] if isinstance(ctx['class_names'], (list, tuple)) else []
                                lines = []
                                if boxes is not None and len(boxes) > 0:
                                    count = boxes.shape[0]
                                    for i in range(count):
                                        x1, y1, x2, y2 = [int(v) for v in boxes[i].tolist()]
                                        conf_v = float(scores[i]) if scores is not None else 0.0
                                        cls_v = int(cls_ids[i]) if cls_ids is not None else 0
                                        label = names[cls_v] if isinstance(names, (list, tuple)) and 0 <= cls_v < len(names) else f'class{cls_v}'
                                        cv2.rectangle(annotated_bgr, (x1, y1), (x2, y2), (0, 200, 255), 2)
                                        cv2.putText(annotated_bgr, f"{label} {conf_v:.2f}", (x1, max(0, y1 - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 200, 255), 2)
                                        ts_ms = int(time.time() * 1000)
                                        ts = time.strftime("%H:%M:%S", time.localtime(ts_ms / 1000)) + f".{ts_ms % 1000:03d}"
                                        lines.append(f"{i + 1}. æ—¶é—´ {ts} | ç±»åˆ« {label} | ä½ç½® ({x1},{y1},{x2},{y2}) | ç½®ä¿¡åº¦ {conf_v:.2f}")
                                if lines:
                                    self.meta_ready.emit("\n".join(lines))
                            except Exception:
                                # è‹¥v1æµç¨‹å¼‚å¸¸ï¼Œä¸é˜»å¡æ•´ä½“çº¿ç¨‹
                                annotated_bgr = frame
                        else:
                            # æœªçŸ¥æ¨¡å‹ç±»å‹ï¼Œç›´æ¥è·³è¿‡
                            annotated_bgr = frame

                    annotated_rgb = cv2.cvtColor(annotated_bgr, cv2.COLOR_BGR2RGB)
                    h, w, ch = annotated_rgb.shape
                    # æ„é€ ç‹¬ç«‹QImageï¼Œé˜²æ­¢æŒ‡é’ˆæ‚¬æŒ‚
                    qt_img = QImage(annotated_rgb.copy().data, w, h, ch * w, QImage.Format_RGB888).copy()
                    self.frame_ready.emit(qt_img)
                    self._last_infer_ms = now_ms
                finally:
                    self._busy = False
            except Exception:
                # å‡ºé”™æ—¶çŸ­æš‚ä¼‘çœ ï¼Œé¿å…busy loop
                self.msleep(5)


class AutonomousDrivingUI(QMainWindow, AutonomousDrivingUISetup, CameraVideoHandler):
    """ç›®æ ‡æ£€æµ‹ç³»ç»Ÿä¸»çª—å£"""

    def __init__(self):
        super().__init__()
        # åˆå§‹åŒ–UI
        self.setup_ui()
        # åˆå§‹åŒ–æ‘„åƒå¤´å’Œè§†é¢‘å¤„ç†
        self.setup_camera_video()

        # æ£€æµ‹å¼€å…³ï¼ˆå¼€å§‹/åœæ­¢ï¼‰
        self.detecting = False
        # æ¨ç†èŠ‚æµä¸åˆ†è¾¨ç‡
        self.infer_interval_ms = 150  # æœ€å°æ¨ç†é—´éš”ï¼Œé˜²æ­¢é¢‘ç¹å¡é¡¿
        self.last_infer_ts = 0
        self.infer_size = 512  # CPU é»˜è®¤ä½¿ç”¨è¾ƒä½åˆ†è¾¨ç‡ï¼ŒGPU ä¼šåœ¨åŠ è½½åè‡ªåŠ¨æå‡
        self._infer_busy = False
        # æŒ‰å¸§æ£€æµ‹ï¼ˆç¡®ä¿è¿ç»­å¤šå¸§åŠ¨æ€æ£€æµ‹ï¼‰
        self.frame_skip = 2  # CPU é»˜è®¤æ¯2å¸§æ£€æµ‹ä¸€æ¬¡ï¼›GPUä¸‹ä¼šæ”¹ä¸ºæ¯å¸§
        self.video_frame_index = 0
        self.camera_frame_index = 0
        # åå°æ¨ç†çº¿ç¨‹
        self.infer_thread = None
        # æœ€è¿‘ä¸€æ¬¡å åŠ åçš„ç»“æœå¸§ï¼ˆç”¨äºæŒç»­æ˜¾ç¤ºï¼Œé¿å…é—ªçƒï¼‰
        self._last_annotated_qimage = None
        # UI åˆ·æ–°èŠ‚æµï¼ˆæŒ‰ç›®æ ‡å¸§ç‡é™åˆ¶æ ‡ç­¾åˆ·æ–°é¢‘ç‡ï¼‰
        self._last_ui_update_ms = 0
        self.ui_min_update_interval_ms = 1000 // 30

        # åŠ è½½YOLOæ¨¡å‹ï¼ˆv5/v8ï¼‰
        # å¯åŠ¨æ—¶å…ˆåˆ·æ–°ä¸€æ¬¡æœ¬åœ°æƒé‡åˆ—è¡¨ï¼Œé¿å…å¼¹çª—
        try:
            self.refresh_local_weights()
        except Exception:
            pass
        # é»˜è®¤ä¸è‡ªåŠ¨åŠ è½½æ¨¡å‹ï¼Œç­‰å¾…ç”¨æˆ·é€‰æ‹©å…·ä½“æƒé‡
        self.model = None
        self.model_type = 'v5'
        # é€šè¿‡æŒ‰é’®é€‰æ‹©çš„æƒé‡è·¯å¾„
        self.selected_weight_path = None

        # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
        self.recent_detections = []  # å­˜å‚¨æœ€è¿‘æ£€æµ‹ç»“æœ
        self.reset_stats()
        # åˆå§‹ç¦ç”¨å¼€å§‹æ£€æµ‹ï¼Œå¾…æ¨¡å‹æˆåŠŸåŠ è½½åå¯ç”¨
        try:
            if hasattr(self, 'btn_play'):
                self.btn_play.setEnabled(False)
        except Exception:
            pass

    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.update_detection_info(0, 0, 0, 0, {})

    def _get_base_dir(self):
        """è·å–åº”ç”¨çš„åŸºç¡€ç›®å½•ï¼šå¼€å‘ç¯å¢ƒè¿”å›é¡¹ç›®ä¸Šçº§ç›®å½•ï¼Œæ‰“åŒ…ç¯å¢ƒè¿”å›å¯æ‰§è¡Œæ–‡ä»¶ç›®å½•"""
        try:
            if getattr(sys, 'frozen', False):
                return os.path.dirname(sys.executable)
            return os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))
        except Exception:
            return os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))

    def update_detection_info(self, targets, inference_time, fps, progress, class_counts, status="å¾…æœºä¸­"):
        """ä¸å†æ„å»ºå›ºå®šä¿¡æ¯æ–‡æœ¬ï¼Œä¿æŒä¸ç”¨æˆ·è¦æ±‚ä¸€è‡´ï¼Œç”±æ¨ç†çº¿ç¨‹è¿½åŠ æ—¥å¿—"""
        return

    def get_recent_detections(self):
        """è·å–æœ€è¿‘æ£€æµ‹ç»“æœ"""
        if hasattr(self, 'recent_detections') and self.recent_detections:
            return "\n   ".join(self.recent_detections[-3:])  # æ˜¾ç¤ºæœ€è¿‘3æ¡
        return "æš‚æ— æ£€æµ‹æ•°æ®"

    def get_system_tip(self, status):
        """è·å–ç³»ç»Ÿæç¤º"""
        tips = {
            "å¾…æœºä¸­": "è¯·é€‰æ‹©è¾“å…¥æºå¼€å§‹æ£€æµ‹",
            "æ£€æµ‹ä¸­": "æ­£åœ¨å¤„ç†è¾“å…¥æ•°æ®ï¼Œè¯·ç¨å€™...",
            "å®Œæˆ": "æ£€æµ‹å®Œæˆï¼Œå¯æŸ¥çœ‹ç»“æœ",
            "é”™è¯¯": "æ£€æµ‹è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œè¯·æ£€æŸ¥è¾“å…¥"
        }
        return tips.get(status, "ç³»ç»Ÿè¿è¡Œæ­£å¸¸")

    def connect_signals(self):
        """è¿æ¥æ‰€æœ‰ä¿¡å·å’Œæ§½å‡½æ•°"""
        # å‚æ•°æ§åˆ¶ä¿¡å· - ç°åœ¨é€šè¿‡æŒ‰é’®å¼¹çª—å¤„ç†ï¼Œä¸éœ€è¦ç›´æ¥è¿æ¥
        # self.conf_spin.valueChanged.connect(self.on_conf_changed)
        # self.conf_slider.valueChanged.connect(self.on_conf_slider_changed)
        # self.iou_spin.valueChanged.connect(self.on_iou_changed)
        # self.iou_slider.valueChanged.connect(self.on_iou_slider_changed)

        # è¾“å…¥æºé€‰æ‹©ä¿¡å·
        self.btn_image.clicked.connect(self.on_image_clicked)
        self.btn_video.clicked.connect(self.on_video_clicked)
        self.btn_camera.clicked.connect(self.on_camera_clicked)

        # æ¨¡å‹é€‰æ‹©ä¿¡å·
        try:
            if hasattr(self, 'btn_select_weight'):
                self.btn_select_weight.clicked.connect(self.on_select_weight_clicked)
        except Exception:
            pass

        # æ§åˆ¶ä¿¡å·
        self.btn_play.clicked.connect(self.on_play_clicked)
        self.btn_stop.clicked.connect(self.on_stop_clicked)

    # å‚æ•°è°ƒæ•´å›è°ƒå‡½æ•°
    def on_conf_changed(self, value):
        """Confidenceå€¼æ”¹å˜"""
        self.conf_slider.setValue(int(value * 100))
        self.console.append(f"ğŸ¯ Confidenceé˜ˆå€¼è°ƒæ•´ä¸º: {value:.2f}")

    def on_conf_slider_changed(self, value):
        """Confidenceæ»‘å—æ”¹å˜"""
        self.conf_spin.setValue(value / 100.0)

    def on_iou_changed(self, value):
        """IOUå€¼æ”¹å˜"""
        self.iou_slider.setValue(int(value * 100))
        self.console.append(f"ğŸ“ IOUé˜ˆå€¼è°ƒæ•´ä¸º: {value:.2f}")

    def on_iou_slider_changed(self, value):
        """IOUæ»‘å—æ”¹å˜"""
        self.iou_spin.setValue(value / 100.0)

    def load_model(self):
        """åŠ è½½YOLOæ¨¡å‹ï¼ˆv5 æˆ– v8ï¼Œä¼˜å…ˆæœ¬åœ°ä»“åº“ï¼‰ï¼Œæ”¯æŒè‡ªå®šä¹‰æƒé‡"""
        # æ¨æµ‹æœ¬åœ°ä»“åº“æ ¹ï¼šå¼€å‘ç¯å¢ƒ=é¡¹ç›®ä¸Šçº§ï¼Œæ‰“åŒ…ç¯å¢ƒ=exeç›®å½•
        base_dir = self._get_base_dir()
        # v5 ä»“åº“å€™é€‰ï¼šä¼˜å…ˆ F:/.../YOLOv5 ç›®å½•ï¼Œå…¶æ¬¡ YOLOv5/yolov5-masterï¼Œå†æ¬¡ yolo v5 åŒçº§
        v5_repo_candidates = [
            os.path.join(base_dir, 'YOLOv5'),
            os.path.join(base_dir, 'YOLOv5', 'yolov5-master'),
            os.path.join(base_dir, 'yolov5'),
            # æ‰“åŒ…ç¯å¢ƒå¸¸è§å†…ç½®è·¯å¾„
            os.path.join(base_dir, '_internal', 'YOLOv5', 'yolov5-master'),
            os.path.join(base_dir, '_internal', 'YOLOv5'),
        ]
        # v3 ä»“åº“å€™é€‰
        v3_repo_candidates = [
            os.path.join(base_dir, 'YOLOv3'),
            os.path.join(base_dir, 'yolov3'),
            os.path.join(base_dir, '_internal', 'YOLOv3'),
        ]
        # v1 ä»“åº“å€™é€‰
        v1_repo_candidates = [
            os.path.join(base_dir, 'YOLOv1'),
            os.path.join(base_dir, 'yolov1'),
            os.path.join(base_dir, '_internal', 'YOLOv1'),
        ]
        # ä»…å½“ç›®å½•å†…å« hubconf.py æ‰è§†ä¸ºæœ‰æ•ˆ YOLOv5/YOLOv3 æœ¬åœ°ä»“åº“
        v5_repo = next((p for p in v5_repo_candidates if os.path.isfile(os.path.join(p, 'hubconf.py'))), None)
        v3_repo = next((p for p in v3_repo_candidates if os.path.isfile(os.path.join(p, 'hubconf.py'))), None)
        # v1 ä»…éœ€å­˜åœ¨æ ¸å¿ƒæ¨¡å—æ–‡ä»¶åˆ¤å®š
        v1_repo = next((p for p in v1_repo_candidates if os.path.isfile(os.path.join(p, 'NetModel.py'))), None)
        # é¿å… torch.hub ç¼“å­˜å¯¼è‡´åŠ è½½æ—§ä»£ç ï¼Œå¼ºåˆ¶æœ¬åœ°ä¼˜å…ˆæ—¶å¯å…³é—­ç¼“å­˜
        try:
            torch.hub.set_dir(os.path.join(base_dir, '.torchhub'))
        except Exception:
            pass
        if v5_repo is None:
            self.console.append("âš ï¸ æœªæ‰¾åˆ°æœ¬åœ°YOLOv5ä»“åº“ï¼Œå°†å°è¯•è”ç½‘åŠ è½½ï¼ˆå¯èƒ½è¾ƒæ…¢ï¼‰")

        # ä½¿ç”¨æŒ‰é’®é€‰æ‹©çš„æƒé‡è·¯å¾„
        weights_path = getattr(self, 'selected_weight_path', None)
        sp = (weights_path or '').strip()
        sp_lower = os.path.basename(sp).lower()
        path_lower = (sp or '').lower()
        # å…ˆåŸºäºè·¯å¾„ååšåå¥½åˆ¤æ–­ï¼›è‹¥ä¸ç¡®å®šåˆ™è‡ªåŠ¨å°è¯•
        prefer_v8 = ('yolov8' in path_lower) or ('v8' in sp_lower)
        prefer_v5 = ('yolov5' in path_lower) or ('v5' in sp_lower)
        prefer_v3 = ('yolov3' in path_lower) or ('v3' in sp_lower)
        prefer_v1 = ('yolov1' in path_lower) or ('v1' in sp_lower)

        try:
            self.console.append(f"ğŸ§© Torch ç‰ˆæœ¬: {getattr(torch, '__version__', 'unknown')}")
            self.console.append(f"ğŸ“¦ æƒé‡è·¯å¾„: {weights_path if (weights_path and os.path.isfile(weights_path)) else 'æœªé€‰æ‹©'}")

            if not weights_path or not os.path.isfile(weights_path):
                self.console.append("â„¹ï¸ æœªé€‰æ‹©æƒé‡ï¼Œè·³è¿‡æ¨¡å‹åŠ è½½")
                if hasattr(self, 'lbl_model_status'):
                    self.lbl_model_status.setText("âŒ æ¨¡å‹æœªåŠ è½½")
                if hasattr(self, 'btn_play'):
                    self.btn_play.setEnabled(False)
                return None

            model = None

            # å®šä¹‰åŠ è½½å™¨ï¼ˆå…ˆå°è¯• v8ï¼Œå¤±è´¥å†å°è¯• v5ï¼›è‹¥æŒ‡å®š forced åˆ™æŒ‰æŒ‡å®šåŠ è½½ï¼‰
            def _try_load_v8(p):
                from ultralytics import YOLO
                return YOLO(p)

            def _try_load_v5(p):
                if v5_repo and os.path.isdir(v5_repo):
                    try:
                        return torch.hub.load(v5_repo, 'custom', path=p, source='local', trust_repo=True)
                    except TypeError:
                        return torch.hub.load(v5_repo, 'custom', path=p, source='local')
                else:
                    try:
                        return torch.hub.load('ultralytics/yolov5', 'custom', path=p, trust_repo=True)
                    except TypeError:
                        return torch.hub.load('ultralytics/yolov5', 'custom', path=p)

            def _try_load_v3(p):
                # æœ¬åœ° YOLOv3 ä»“åº“é€šè¿‡ hubconf.py çš„ custom å®ç°
                if v3_repo and os.path.isdir(v3_repo):
                    try:
                        return torch.hub.load(v3_repo, 'custom', path=p, source='local', trust_repo=True)
                    except TypeError:
                        return torch.hub.load(v3_repo, 'custom', path=p, source='local')
                else:
                    # è‹¥ç¼ºå¤±æœ¬åœ°ä»“åº“ï¼Œä¸åšè”ç½‘åŠ è½½ï¼Œä¸¥æ ¼åŒ¹é…ç›´æ¥æŠ¥é”™
                    raise RuntimeError('æœªæ‰¾åˆ°æœ¬åœ° YOLOv3 ä»“åº“ (ç¼ºå°‘ hubconf.py)')

            def _try_load_v1(p):
                # åŠ¨æ€å¯¼å…¥ YOLOv1 å·¥ç¨‹ä»£ç 
                if not v1_repo or not os.path.isdir(v1_repo):
                    raise RuntimeError('æœªæ‰¾åˆ°æœ¬åœ° YOLOv1 å·¥ç¨‹ (ç¼ºå°‘ NetModel.py)')
                import importlib, types
                if v1_repo not in sys.path:
                    sys.path.insert(0, v1_repo)
                NetModel = importlib.import_module('NetModel')
                Utils = importlib.import_module('Utils')
                # è¯»å–é»˜è®¤è¶…å‚
                try:
                    hyper = Utils.load_hyperparameters()
                except Exception:
                    hyper = {'input_size': 448, 'grid_size': 7, 'num_classes': 20}
                input_size = int(hyper.get('input_size', 448))
                grid_size = int(hyper.get('grid_size', 7))
                num_classes = int(hyper.get('num_classes', 20))
                # æ„å»ºæ¨¡å‹
                model = NetModel.YOLOv1(
                    class_num=num_classes,
                    grid_size=grid_size,
                    training_mode='detection',
                    input_size=input_size,
                    use_efficient_backbone=True
                )
                device = 'cuda' if torch.cuda.is_available() else 'cpu'
                model.to(device)
                model.eval()
                # åŠ è½½æƒé‡ï¼ˆå…¼å®¹åŒ…å«'æ¨¡å‹é”®'æˆ–ç›´æ¥ state_dict çš„æƒ…å†µï¼‰
                try:
                    add_safe_globals = getattr(torch.serialization, 'add_safe_globals', None)
                    if add_safe_globals is not None:
                        try:
                            import numpy as _np
                            add_safe_globals([_np.core.multiarray.scalar])
                        except Exception:
                            pass
                    ckpt = torch.load(p, map_location=device)
                except Exception:
                    # å›é€€ï¼šæ˜ç¡®å…è®¸ä¸å®‰å…¨ååºåˆ—åŒ–ï¼ˆä»…åœ¨å¯ä¿¡æ¥æºä¸‹ï¼‰
                    ckpt = torch.load(p, map_location=device, weights_only=False)

                # å…¼å®¹å„ç§å¸¸è§é”®å
                state = None
                if isinstance(ckpt, dict):
                    for _k in ('model_state_dict', 'state_dict', 'weights', 'params'):
                        if _k in ckpt:
                            state = ckpt[_k]
                            break
                    if state is None:
                        state = ckpt
                else:
                    state = ckpt
                model.load_state_dict(state, strict=False)
                # ç±»åˆ«åä¸è§£ç å‡½æ•°
                try:
                    class_names = Utils.create_class_names('voc')
                except Exception:
                    class_names = [str(i) for i in range(num_classes)]
                decode_func = Utils.decode_yolo_output
                # è¿”å›æ¨¡å‹ä¸v1ä¸Šä¸‹æ–‡
                v1_ctx = {
                    'input_size': input_size,
                    'grid_size': grid_size,
                    'num_classes': num_classes,
                    'class_names': class_names,
                    'decode_func': decode_func,
                    'device': device,
                }
                return model, v1_ctx

            forced = getattr(self, 'forced_model_type', None)
            # ä¸¥æ ¼åŒ¹é…ï¼šä»…æŒ‰æŒ‡å®šæˆ–å¯æ˜ç¡®æ¨æ–­çš„ç±»å‹åŠ è½½ï¼Œä¸åšè·¨ç‰ˆæœ¬å›é€€
            target_type = None
            if forced in ('v8', 'v5'):
                target_type = forced
            else:
                # åŸºäºè·¯å¾„å…³é”®è¯çš„æœ€å°åŒ–æ¨æ–­ï¼ˆä»…ç”¨äºé˜²è¯¯åˆ¤ï¼Œä¸åšè·¨åŠ è½½ï¼‰
                if 'yolov8' in path_lower or 'ultralytics' in path_lower or prefer_v8:
                    target_type = 'v8'
                elif 'yolov5' in path_lower or prefer_v5:
                    target_type = 'v5'
                elif 'yolov3' in path_lower or prefer_v3:
                    target_type = 'v3'
                elif 'yolov1' in path_lower or prefer_v1:
                    target_type = 'v1'
                else:
                    self.console.append("âŒ æœªèƒ½æ˜ç¡®è¯†åˆ«è¯¥æƒé‡çš„æ¨¡å‹ç±»å‹ã€‚ä¸ºé¿å…é”™è¯¯åŠ è½½ï¼Œå·²å–æ¶ˆæœ¬æ¬¡åŠ è½½ã€‚è¯·å°†æƒé‡æ”¾åœ¨ YOLOv5 æˆ– YOLOv8 ç›®å½•ï¼Œæˆ–åœ¨ä»£ç ä¸­è®¾ç½® forced_model_typeã€‚")
                    if hasattr(self, 'btn_play'):
                        self.btn_play.setEnabled(False)
                    return None

            if target_type == 'v8':
                model = _try_load_v8(weights_path)
                self.model_type = 'v8'
                self.console.append(f"âœ… å·²æŒ‰ YOLOv8 åŠ è½½: {os.path.basename(weights_path)}")
            elif target_type == 'v5':
                model = _try_load_v5(weights_path)
                self.model_type = 'v5'
                self.console.append(f"âœ… å·²æŒ‰ YOLOv5 åŠ è½½: {os.path.basename(weights_path)}")
            elif target_type == 'v3':
                model = _try_load_v3(weights_path)
                self.model_type = 'v3'
                self.console.append(f"âœ… å·²æŒ‰ YOLOv3 åŠ è½½: {os.path.basename(weights_path)}")
            elif target_type == 'v1':
                model, v1_ctx = _try_load_v1(weights_path)
                self.model_type = 'v1'
                # ä¿å­˜åˆ°å®ä¾‹ï¼Œä¾›æ¨ç†çº¿ç¨‹ä½¿ç”¨
                self._v1_ctx = v1_ctx
                self.console.append(f"âœ… å·²æŒ‰ YOLOv1 åŠ è½½: {os.path.basename(weights_path)}")
            else:
                raise RuntimeError('æœªçŸ¥çš„æ¨¡å‹ç±»å‹')

            # è®¾ç½®é˜ˆå€¼ï¼ˆä¸UIä¿æŒä¸€è‡´ï¼‰ï¼Œå…¼å®¹ä¸åŒç‰ˆæœ¬API
            try:
                if self.model_type == 'v8':
                    # YOLOv8 ä½¿ç”¨æ¨ç†å‚æ•°æ§åˆ¶ï¼Œæ— éœ€åœ¨æ¨¡å‹ä¸Šè®¾ç½®
                    pass
                else:
                    if hasattr(self, 'confidence_value') and hasattr(model, 'conf'):
                        model.conf = float(self.confidence_value)
                    if hasattr(self, 'iou_value') and hasattr(model, 'iou'):
                        model.iou = float(self.iou_value)
            except Exception:
                pass

            # æ¨ç†è®¾å¤‡
            try:
                device = 'cuda' if torch.cuda.is_available() else 'cpu'
                if self.model_type in ('v5', 'v3'):
                    model.to(device)
                else:
                    try:
                        model.to(device)
                    except Exception:
                        pass
                self.console.append(f"ğŸ–¥ï¸ æ¨ç†è®¾å¤‡: {device}")
                # GPU ç”¨é«˜åˆ†è¾¨ç‡ï¼ŒCPU ç”¨è¾ƒä½åˆ†è¾¨ç‡
                self.infer_size = 640 if device == 'cuda' else 512
                self.frame_skip = 1 if device == 'cuda' else 2
            except Exception:
                pass

            # æ¨¡å‹é¢„çƒ­ï¼šé¿å…é¦–å¸§æ¨ç†å¡é¡¿
            try:
                import numpy as _np
                dummy = _np.zeros((self.infer_size, self.infer_size, 3), dtype=_np.uint8)
                with torch.no_grad():
                    if self.model_type == 'v8':
                        _ = model(dummy, imgsz=int(self.infer_size), conf=float(getattr(self, 'confidence_value', 0.5)), iou=float(getattr(self, 'iou_value', 0.45)))
                    elif self.model_type in ('v5', 'v3'):
                        _ = model(dummy, size=int(self.infer_size))
                    else:
                        # v1 é¢„çƒ­ä¸€æ¬¡
                        import torch as _t
                        _ = model(_t.from_numpy(dummy).float().permute(2, 0, 1).unsqueeze(0).to(self._v1_ctx.get('device', 'cpu')))
            except Exception:
                pass

            # æ›´æ–°æ¨¡å‹çŠ¶æ€æ ‡ç­¾
            if hasattr(self, 'lbl_model_status'):
                mt = getattr(self, 'model_type', 'v5').upper()
                self.lbl_model_status.setText(f"ğŸ“‹ æ¨¡å‹å°±ç»ª (YOLO{mt})")

            return model
        except Exception as e:
            self.console.append(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            return None

    def detect_image(self, image_path):
        """ä½¿ç”¨YOLOv5/YOLOv8æ£€æµ‹å›¾åƒå¹¶æ˜¾ç¤ºæ ‡æ³¨ç»“æœï¼ˆæ”¯æŒä¸­æ–‡è·¯å¾„ï¼‰"""
        try:
            if self.model is None:
                self.console.append("âš ï¸ æœªåŠ è½½æ¨¡å‹ï¼Œæ— æ³•è¿›è¡Œæ£€æµ‹")
                return

            # è¯»å–åŸå§‹å›¾åƒï¼ˆBGRï¼‰ï¼Œæ”¯æŒä¸­æ–‡è·¯å¾„
            bgr = cv2.imdecode(np.fromfile(image_path, dtype=np.uint8), cv2.IMREAD_COLOR)
            if bgr is None:
                self.console.append(f"âŒ æ— æ³•è¯»å–å›¾åƒæ–‡ä»¶: {image_path}")
                return

            start_ts = time.time()
            # è®¾ç½®é˜ˆå€¼çƒ­æ›´æ–°ï¼ˆå¯¹äºv5è®¾ç½®åˆ°æ¨¡å‹ï¼Œå¯¹äºv8ä½¿ç”¨æ¨ç†å‚æ•°ï¼‰
            try:
                if getattr(self, 'model_type', 'v5') == 'v5':
                    if hasattr(self, 'confidence_value') and hasattr(self.model, 'conf'):
                        self.model.conf = float(self.confidence_value)
                    if hasattr(self, 'iou_value') and hasattr(self.model, 'iou'):
                        self.model.iou = float(self.iou_value)
            except Exception:
                pass

            # æ¨ç†ï¼ˆèŠ‚æµä¸éœ€è¦ï¼Œå› ä¸ºå•æ¬¡å›¾åƒæ£€æµ‹ï¼‰
            with torch.no_grad():
                mt = getattr(self, 'model_type', 'v5')
                if mt == 'v8':
                    results = self.model(bgr, imgsz=int(self.infer_size), conf=float(getattr(self, 'confidence_value', 0.5)), iou=float(getattr(self, 'iou_value', 0.45)))
                    rendered = results[0].plot()
                elif mt in ('v5', 'v3'):
                    results = self.model(bgr, size=int(self.infer_size))
                    rendered = results.render()[0]
                elif mt == 'v1' and hasattr(self, '_v1_ctx') and self._v1_ctx:
                    # ä½¿ç”¨ä¸çº¿ç¨‹ä¸€è‡´çš„é¢„å¤„ç†ä¸è§£ç 
                    ctx = self._v1_ctx
                    input_size = int(ctx.get('input_size', 448))
                    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)
                    h0, w0 = rgb.shape[:2]
                    max_edge = max(h0, w0)
                    top = (max_edge - h0) // 2
                    bottom = max_edge - h0 - top
                    left = (max_edge - w0) // 2
                    right = max_edge - w0 - left
                    padded = cv2.copyMakeBorder(rgb, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])
                    resized = cv2.resize(padded, (input_size, input_size))
                    tensor = torch.from_numpy(resized).float().permute(2, 0, 1) / 255.0
                    mean = torch.tensor([0.408, 0.448, 0.471]).view(3, 1, 1)
                    std = torch.tensor([0.242, 0.239, 0.234]).view(3, 1, 1)
                    tensor = (tensor - mean) / std
                    tensor = tensor.unsqueeze(0).to(ctx.get('device', 'cpu'))
                    preds = self.model(tensor)
                    decoded_list = ctx['decode_func'](
                        preds.detach().cpu(),
                        conf_threshold=float(getattr(self, 'confidence_value', 0.5)),
                        nms_threshold=float(getattr(self, 'iou_value', 0.45)),
                        input_size=input_size,
                        grid_size=int(ctx.get('grid_size', 7)),
                        num_classes=int(ctx.get('num_classes', 20))
                    )
                    det = decoded_list[0]
                    boxes = det.get('boxes', None)
                    scores = det.get('scores', None)
                    cls_ids = det.get('class_ids', None)
                    if boxes is not None and len(boxes) > 0:
                        scale = max_edge / input_size
                        boxes = boxes.copy() * scale
                        boxes[:, [0, 2]] -= left
                        boxes[:, [1, 3]] -= top
                        boxes[:, [0, 2]] = np.clip(boxes[:, [0, 2]], 0, w0)
                        boxes[:, [1, 3]] = np.clip(boxes[:, [1, 3]], 0, h0)
                    rendered = bgr.copy()
                    names = ctx.get('class_names', [])
                    if boxes is not None and len(boxes) > 0:
                        for i in range(boxes.shape[0]):
                            x1, y1, x2, y2 = [int(v) for v in boxes[i].tolist()]
                            conf_v = float(scores[i]) if scores is not None else 0.0
                            cls_v = int(cls_ids[i]) if cls_ids is not None else 0
                            label = names[cls_v] if isinstance(names, (list, tuple)) and 0 <= cls_v < len(names) else f'class{cls_v}'
                            cv2.rectangle(rendered, (x1, y1), (x2, y2), (0, 200, 255), 2)
                            cv2.putText(rendered, f"{label} {conf_v:.2f}", (x1, max(0, y1 - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 200, 255), 2)
                else:
                    self.console.append("âŒ æœªçŸ¥çš„æ¨¡å‹ç±»å‹ï¼Œæ— æ³•è¿›è¡Œæ£€æµ‹")
                    return
            inference_ms = int((time.time() - start_ts) * 1000)

            # æ˜¾ç¤ºåˆ°UIï¼ˆè½¬RGBå†æ˜¾ç¤ºï¼‰
            rgb = cv2.cvtColor(rendered, cv2.COLOR_BGR2RGB)
            h, w, ch = rgb.shape
            qt_img = QImage(rgb.data, w, h, ch * w, QImage.Format_RGB888)
            pixmap = QPixmap.fromImage(qt_img)
            scaled_pixmap = pixmap.scaled(self.lbl_video.size(), Qt.KeepAspectRatio, Qt.SmoothTransformation)
            self.lbl_video.setPixmap(scaled_pixmap)

            # è§£æç»“æœå¹¶æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            if getattr(self, 'model_type', 'v5') == 'v1':
                # æ„é€ ä¸ _parse_yolo_results ä¸€è‡´çš„è®¡æ•°
                total = 0
                class_counts = {}
                recent_text = "æ— æ£€æµ‹"
                if 'boxes' in det and det['boxes'] is not None and len(det['boxes']) > 0:
                    total = int(det['boxes'].shape[0])
                    names = self._v1_ctx.get('class_names', [])
                    if 'class_ids' in det and det['class_ids'] is not None:
                        import numpy as _np
                        cls = det['class_ids']
                        unique_cls = _np.unique(cls)
                        for ci in unique_cls:
                            en = names[int(ci)] if isinstance(names, (list, tuple)) else f'class{int(ci)}'
                            class_counts[en] = int((_np.array(cls) == ci).sum())
                    if 'scores' in det and det['scores'] is not None and len(det['scores']) > 0:
                        import numpy as _np
                        conf = det['scores']
                        cls = det['class_ids'] if det.get('class_ids') is not None else [0] * len(conf)
                        top_idx = _np.argsort(-conf)[:3]
                        labels = []
                        for i in top_idx:
                            en = names[int(cls[i])] if isinstance(names, (list, tuple)) else f'class{int(cls[i])}'
                            labels.append(f"{en} {conf[i]:.2f}")
                        if labels:
                            recent_text = ", ".join(labels)
                targets, recent = total, recent_text
            else:
                targets, class_counts, recent_text = self._parse_yolo_results(results)
            self.recent_detections.append(recent_text)
            self.update_detection_info(targets, inference_ms, 0, 100, class_counts, status="å®Œæˆ")

            # ä¿å­˜ç»“æœï¼šç»Ÿä¸€ç”±ä¿å­˜ç›®å½•å†³å®š
            if hasattr(self, 'cb_save') and self.cb_save.isChecked():
                if not getattr(self, 'save_image_dir', None):
                    self._prompt_save_image_dir()
                if getattr(self, 'save_image_dir', None):
                    base = os.path.splitext(os.path.basename(image_path))[0]
                    save_path = os.path.join(self.save_image_dir, base + "_det.jpg")
                    cv2.imencode('.jpg', rendered)[1].tofile(save_path)
                    self.console.append(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {os.path.basename(save_path)}")

            self.console.append("âœ… å›¾åƒæ£€æµ‹å®Œæˆ")

        except Exception as e:
            self.console.append(f"âŒ å›¾åƒæ£€æµ‹å¤±è´¥: {str(e)}")

    def _parse_yolo_results(self, results):
        """ä»YOLO(v5/v8)ç»“æœä¸­ç»Ÿè®¡ç›®æ ‡æ•°ä¸ç±»åˆ«åˆ†å¸ƒï¼Œè¿”å›(æ€»æ•°, ç±»åˆ«ç»Ÿè®¡, æœ€è¿‘ç»“æœæ–‡æœ¬)"""
        try:
            if getattr(self, 'model_type', 'v5') == 'v8':
                res0 = results[0] if isinstance(results, (list, tuple)) else results
                boxes = getattr(res0, 'boxes', None)
                names = getattr(res0, 'names', getattr(self.model, 'names', {}))
                if boxes is not None and hasattr(boxes, 'cls') and boxes.cls is not None:
                    total = int(boxes.cls.shape[0])
                else:
                    total = 0
            else:
                # v5
                pred = results.xyxy[0]  # tensor: [N, 6]
                total = int(pred.shape[0]) if pred is not None else 0

            name_map = {
                'car': 'è½¦è¾†',
                'person': 'è¡Œäºº',
                'traffic light': 'äº¤é€šç¯',
                'stop sign': 'æ ‡å¿—ç‰Œ',
                'bus': 'å…¬äº¤è½¦',
                'truck': 'å¡è½¦',
                'motorcycle': 'æ‘©æ‰˜è½¦',
                'bicycle': 'è‡ªè¡Œè½¦',
            }

            class_counts = {}
            recent_text = "æ— æ£€æµ‹"
            if total > 0:
                import numpy as _np
                if getattr(self, 'model_type', 'v5') == 'v8':
                    boxes = results[0].boxes
                    conf = boxes.conf.detach().cpu().numpy() if hasattr(boxes, 'conf') else _np.array([])
                    cls = boxes.cls.detach().cpu().numpy().astype(int) if hasattr(boxes, 'cls') else _np.array([], dtype=int)
                    names = getattr(results[0], 'names', getattr(self.model, 'names', {}))
                else:
                    pred = results.xyxy[0]
                    conf = pred[:, 4].detach().cpu().numpy()
                    cls = pred[:, 5].detach().cpu().numpy().astype(int)
                    names = getattr(results, 'names', getattr(self.model, 'names', {}))

                unique_cls = _np.unique(cls)
                for ci in unique_cls:
                    en = names[int(ci)] if isinstance(names, (list, dict)) else f'class{int(ci)}'
                    cn = name_map.get(en, en)
                    class_counts[cn] = int((_np.array(cls) == ci).sum())

                # å–ç½®ä¿¡åº¦æœ€é«˜çš„å‰3ä¸ª
                if conf.size > 0:
                    top_idx = _np.argsort(-conf)[:3]
                    labels = []
                    for i in top_idx:
                        en = names[int(cls[i])] if isinstance(names, (list, dict)) else f'class{int(cls[i])}'
                        cn = name_map.get(en, en)
                        labels.append(f"{cn} {conf[i]:.2f}")
                    if labels:
                        recent_text = ", ".join(labels)

            return total, class_counts, recent_text
        except Exception:
            return 0, {}, "è§£æå¤±è´¥"

    # è¦†ç›–æ‘„åƒå¤´å¸§æ˜¾ç¤ºä»¥åŠ å…¥å®æ—¶æ£€æµ‹ï¼ˆå¯ç”±å¼€å§‹/åœæ­¢æŒ‰é’®æ§åˆ¶ï¼‰
    def show_real_video(self, h, w, c, data):
        try:
            frame_bgr = np.frombuffer(data, dtype=np.uint8).reshape((h, w, c))
            # å°†å¸§é€å…¥åå°çº¿ç¨‹è¿›è¡Œæ¨ç†
            if self.detecting and self.model is not None and self.infer_thread is not None:
                self.infer_thread.submit_frame(frame_bgr)

            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)
            raw_qimage = QImage(frame_rgb.data, w, h, c * w, QImage.Format_RGB888)

            # æ£€æµ‹ä¸­ä¼˜å…ˆæ˜¾ç¤ºæœ€è¿‘çš„å åŠ ç»“æœï¼Œé¿å…åŸå§‹å¸§è¦†ç›–é€ æˆâ€œé—ªçƒâ€ï¼›å¹¶é™åˆ¶åˆ·æ–°é¢‘ç‡
            now_ms = int(time.time() * 1000)
            if now_ms - self._last_ui_update_ms >= self.ui_min_update_interval_ms:
                if self.detecting and self._last_annotated_qimage is not None:
                    pixmap = QPixmap.fromImage(self._last_annotated_qimage)
                else:
                    pixmap = QPixmap.fromImage(raw_qimage)
                scaled_pixmap = pixmap.scaled(self.lbl_video.size(), Qt.KeepAspectRatio, Qt.FastTransformation)
                self.lbl_video.setPixmap(scaled_pixmap)
                self._last_ui_update_ms = now_ms
        except Exception as e:
            print(f"æ˜¾ç¤ºæ‘„åƒå¤´ç”»é¢é”™è¯¯: {e}")

    def display_video_frame(self, qt_image):
        """è§†é¢‘æ’­æ”¾å¸§å›è°ƒï¼šåœ¨å®æ—¶æ£€æµ‹å¼€å¯æ—¶æ‰§è¡ŒYOLOv5æ¨ç†å¹¶å åŠ ç»“æœ"""
        try:
            w = qt_image.width()
            h = qt_image.height()
            ch = 3

            # å°†å¸§é€å…¥åå°çº¿ç¨‹è¿›è¡Œæ¨ç†
            if self.detecting and self.model is not None and self.infer_thread is not None:
                bits = qt_image.bits()
                bits.setsize(h * w * ch)
                rgb = np.frombuffer(bits, dtype=np.uint8).reshape((h, w, ch))
                bgr = cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)
                self.infer_thread.submit_frame(bgr)

            # æ£€æµ‹ä¸­ä¼˜å…ˆæ˜¾ç¤ºæœ€è¿‘çš„å åŠ ç»“æœï¼Œé¿å…åŸå§‹å¸§è¦†ç›–é€ æˆâ€œé—ªçƒâ€ï¼›å¹¶é™åˆ¶åˆ·æ–°é¢‘ç‡
            now_ms = int(time.time() * 1000)
            if now_ms - self._last_ui_update_ms >= self.ui_min_update_interval_ms:
                if self.detecting and self._last_annotated_qimage is not None:
                    pixmap = QPixmap.fromImage(self._last_annotated_qimage)
                else:
                    pixmap = QPixmap.fromImage(qt_image)
                scaled_pixmap = pixmap.scaled(self.lbl_video.size(), Qt.KeepAspectRatio, Qt.FastTransformation)
                self.lbl_video.setPixmap(scaled_pixmap)
                self._last_ui_update_ms = now_ms
        except Exception as e:
            print(f"æ˜¾ç¤ºè§†é¢‘å¸§é”™è¯¯: {e}")

    # è¾“å…¥æºé€‰æ‹©å›è°ƒå‡½æ•°
    def on_image_clicked(self):
        """å›¾åƒæ£€æµ‹æŒ‰é’®ç‚¹å‡»"""
        file_path, _ = QFileDialog.getOpenFileName(
            self, "é€‰æ‹©å›¾åƒæ–‡ä»¶", "", "å›¾åƒæ–‡ä»¶ (*.png *.jpg *.jpeg *.bmp)"
        )
        if file_path:
            self.stop_all()  # åœæ­¢å…¶ä»–æ¨¡å¼
            self.current_image_path = file_path
            self.console.append(f"ğŸ–¼ï¸ å·²é€‰æ‹©å›¾åƒ: {os.path.basename(file_path)}")
            # å‹¾é€‰ä¿å­˜ç»“æœæ—¶ï¼Œå…ˆç¡®ä¿ä¿å­˜ç›®å½•
            if hasattr(self, 'cb_save') and self.cb_save.isChecked():
                if not getattr(self, 'save_image_dir', None):
                    self._prompt_save_image_dir()
            # æ˜¾ç¤ºå›¾åƒå¹¶è¿›è¡Œæ£€æµ‹
            self.detect_image(file_path)

    def on_video_clicked(self):
        """è§†é¢‘æ£€æµ‹æŒ‰é’®ç‚¹å‡»"""
        file_path, _ = QFileDialog.getOpenFileName(
            self, "é€‰æ‹©è§†é¢‘æ–‡ä»¶", "", "è§†é¢‘æ–‡ä»¶ (*.mp4 *.avi *.mov *.mkv)"
        )
        if file_path:
            self.stop_all()  # åœæ­¢å…¶ä»–æ¨¡å¼
            self.console.append(f"ğŸ¬ å·²é€‰æ‹©è§†é¢‘: {os.path.basename(file_path)}")
            # å‹¾é€‰ä¿å­˜ç»“æœæ—¶ï¼Œå…ˆç¡®ä¿ä¿å­˜ç›®å½•
            if hasattr(self, 'cb_save') and self.cb_save.isChecked():
                if not getattr(self, 'save_image_dir', None):
                    self._prompt_save_image_dir()
            self.start_video_playback(file_path)
            # å¼€å§‹åå°æ¨ç†çº¿ç¨‹
            self._start_infer_thread()

    def on_camera_clicked(self):
        """æ‘„åƒå¤´æŒ‰é’®ç‚¹å‡»"""
        if not self.camera_running:
            # åœæ­¢å…¶ä»–æ¨¡å¼
            self.stop_video_playback()

            # å°è¯•å¯åŠ¨çœŸå®æ‘„åƒå¤´
            if self.start_camera():
                self.console.append("ğŸ“¹ å¯åŠ¨çœŸå®æ‘„åƒå¤´æ£€æµ‹")
                self.btn_camera.setText("ğŸ“¹ åœæ­¢æ‘„åƒå¤´")
                self.btn_camera.setStyleSheet("""
                    QPushButton {
                        background: qlineargradient(x1: 0, y1: 0, x2: 0, y2: 1,
                            stop: 0 #e74c3c, stop: 1 #c0392b);
                        font-size: 13px;
                    }
                """)
                # é‡ç½®ç»Ÿè®¡ä¿¡æ¯
                self.reset_stats()
            else:
                self.console.append("âŒ æ— æ³•å¯åŠ¨æ‘„åƒå¤´")
        else:
            # åœæ­¢æ‘„åƒå¤´
            self.stop_camera()
            self.btn_camera.setText("ğŸ“¹ å®æ—¶æ‘„åƒå¤´")
            self.btn_camera.setStyleSheet("""
                QPushButton {
                    background: qlineargradient(x1: 0, y1: 0, x2: 0, y2: 1,
                        stop: 0 #8e44ad, stop: 1 #6c3483);
                    font-size: 13px;
                }
            """)

    # æ§åˆ¶å›è°ƒå‡½æ•°
    def on_play_clicked(self):
        """å¼€å§‹æ£€æµ‹æŒ‰é’®ç‚¹å‡»"""
        # å¼€å¯æ£€æµ‹æ¨¡å¼ï¼ˆç”¨äºæ‘„åƒå¤´/è§†é¢‘ï¼‰
        self.detecting = True
        self._start_infer_thread()
        # å¦‚å‹¾é€‰ä¿å­˜ç»“æœï¼Œç»Ÿä¸€åœ¨é¦–æ¬¡éœ€è¦æ—¶è¯¢é—®ä¿å­˜ç›®å½•ï¼ˆæ‰€æœ‰æ¨¡å¼é€šç”¨ï¼‰
        if hasattr(self, 'cb_save') and self.cb_save.isChecked():
            if not getattr(self, 'save_image_dir', None):
                self._prompt_save_image_dir()
        if self.camera_running:
            self.console.append("â–¶ï¸ å·²å¯ç”¨æ‘„åƒå¤´å®æ—¶æ£€æµ‹")
        elif self.video_thread and self.video_thread.isRunning():
            self.console.append("â–¶ï¸ å·²å¯ç”¨è§†é¢‘å®æ—¶æ£€æµ‹")
        elif self.current_image_path:
            self.console.append("â–¶ï¸ å¼€å§‹å›¾åƒæ£€æµ‹")
            self.detect_image(self.current_image_path)
        else:
            self.console.append("âš ï¸ è¯·å…ˆé€‰æ‹©è¾“å…¥æº")

    def on_stop_clicked(self):
        """åœæ­¢æ£€æµ‹æŒ‰é’®ç‚¹å‡»"""
        # å…³é—­å®æ—¶æ£€æµ‹
        self.detecting = False
        self.stop_all()
        self._stop_infer_thread()
        self.console.append("â¹ï¸ åœæ­¢æ‰€æœ‰æ£€æµ‹")
        # é‡ç½®ç»Ÿè®¡ä¿¡æ¯
        self.reset_stats()
        # åœæ­¢åä¸å†ä¿ç•™å†å²è¾“å‡ºï¼ˆæ ¹æ®éœ€è¦å¯æ³¨é‡Šæ‰ï¼‰
        self.detection_info.setPlainText("")
        # æ¸…ç©ºå åŠ å¸§ç¼“å­˜
        self._last_annotated_qimage = None
        # æ¸…ç†ä¿å­˜çŠ¶æ€
        self._clear_save_state()

    # å·²ç§»é™¤æ¨¡å‹ç³»åˆ—é€‰æ‹©

    # æ—§çš„ä¸‹æ‹‰åˆ‡æ¢é€»è¾‘å·²åºŸå¼ƒ

    def refresh_local_weights(self):
        """å…¼å®¹æ—§æ¥å£ï¼šä¸å†åˆ·æ–°ä¸‹æ‹‰ï¼›æ”¹ä¸ºæŒ‰é’®é€‰æ‹©æ—¶æ‰“å¼€ç›®å½•æµè§ˆã€‚"""
        return

    def closeEvent(self, event):
        """çª—å£å…³é—­äº‹ä»¶"""
        self.stop_all()
        self._stop_infer_thread()
        self._last_annotated_qimage = None
        event.accept()

    def _start_infer_thread(self):
        """å¯åŠ¨æˆ–æ›´æ–°åå°æ¨ç†çº¿ç¨‹å‚æ•°"""
        if self.model is None:
            return
        if self.infer_thread is None:
            # æ ¹æ®æƒé‡æ–‡ä»¶åæ¨æ–­çº¿ç¨‹çš„æ¨¡å‹ç±»å‹
            model_type = getattr(self, 'model_type', 'v5')
            self.infer_thread = InferenceThread(self.model, self.infer_size, self.frame_skip, self.infer_interval_ms, model_type=model_type, conf=getattr(self, 'confidence_value', 0.5), iou=getattr(self, 'iou_value', 0.45))
            # è‹¥ä¸º v1ï¼Œæ³¨å…¥æ¨ç†ä¸Šä¸‹æ–‡
            try:
                if getattr(self, 'model_type', '') == 'v1' and hasattr(self, '_v1_ctx') and self._v1_ctx:
                    self.infer_thread.set_v1_context(
                        input_size=int(self._v1_ctx.get('input_size', 448)),
                        grid_size=int(self._v1_ctx.get('grid_size', 7)),
                        num_classes=int(self._v1_ctx.get('num_classes', 20)),
                        class_names=self._v1_ctx.get('class_names', []),
                        decode_func=self._v1_ctx.get('decode_func'),
                        device=self._v1_ctx.get('device', 'cpu')
                    )
            except Exception:
                pass
            # ä½¿ç”¨QueuedConnectionç¡®ä¿è·¨çº¿ç¨‹UIä¿¡å·å®‰å…¨
            try:
                from PyQt5.QtCore import Qt as _Qt
                self.infer_thread.frame_ready.connect(self._on_infer_frame_ready, type=_Qt.QueuedConnection)
                self.infer_thread.meta_ready.connect(self._on_infer_meta_ready, type=_Qt.QueuedConnection)
            except Exception:
                self.infer_thread.frame_ready.connect(self._on_infer_frame_ready)
                self.infer_thread.meta_ready.connect(self._on_infer_meta_ready)
            try:
                self.infer_thread.setObjectName("InferenceThread")
            except Exception:
                pass
            self.infer_thread.start()
        else:
            model_type = getattr(self, 'model_type', 'v5')
            self.infer_thread.update_params(self.infer_size, self.frame_skip, self.infer_interval_ms, conf=getattr(self, 'confidence_value', 0.5), iou=getattr(self, 'iou_value', 0.45), model_type=model_type)
            # è‹¥ä¸º v1ï¼Œæ›´æ–°ä¸Šä¸‹æ–‡
            try:
                if getattr(self, 'model_type', '') == 'v1' and hasattr(self, '_v1_ctx') and self._v1_ctx:
                    self.infer_thread.set_v1_context(
                        input_size=int(self._v1_ctx.get('input_size', 448)),
                        grid_size=int(self._v1_ctx.get('grid_size', 7)),
                        num_classes=int(self._v1_ctx.get('num_classes', 20)),
                        class_names=self._v1_ctx.get('class_names', []),
                        decode_func=self._v1_ctx.get('decode_func'),
                        device=self._v1_ctx.get('device', 'cpu')
                    )
            except Exception:
                pass

    def _stop_infer_thread(self):
        if self.infer_thread is not None:
            try:
                self.infer_thread.stop()
                self.infer_thread.wait(2000)
            except Exception:
                pass
            self.infer_thread = None
    
    def _clear_save_state(self):
        try:
            if hasattr(self, 'save_image_dir'):
                del self.save_image_dir
            if hasattr(self, 'save_image_index'):
                del self.save_image_index
        except Exception:
            pass

    def _on_infer_frame_ready(self, qt_image: QImage):
        """æ”¶åˆ°åå°æ¨ç†ç»“æœå¸§ï¼Œéé˜»å¡æ›´æ–°UI"""
        try:
            # ç¼“å­˜æœ€æ–°çš„å åŠ ç»“æœï¼Œä¾›åŸå§‹å¸§åˆ°è¾¾æ—¶ä¹Ÿä¼˜å…ˆæ˜¾ç¤º
            self._last_annotated_qimage = qt_image
            now_ms = int(time.time() * 1000)
            if now_ms - self._last_ui_update_ms >= self.ui_min_update_interval_ms:
                pixmap = QPixmap.fromImage(qt_image)
                scaled_pixmap = pixmap.scaled(self.lbl_video.size(), Qt.KeepAspectRatio, Qt.FastTransformation)
                self.lbl_video.setPixmap(scaled_pixmap)
                self._last_ui_update_ms = now_ms

            # å†™å…¥å›¾ç‰‡åºåˆ—
            if getattr(self, 'save_image_dir', None):
                try:
                    w = qt_image.width()
                    h = qt_image.height()
                    ch = 3
                    tmp_img = qt_image.copy()
                    bits = tmp_img.bits()
                    bits.setsize(h * w * ch)
                    rgb = np.frombuffer(bits, dtype=np.uint8).reshape((h, w, ch))
                    bgr = cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)
                    name = f"frame_{time.strftime('%Y%m%d_%H%M%S')}_{getattr(self, 'save_image_index', 0):06d}.jpg"
                    cv2.imwrite(os.path.join(self.save_image_dir, name), bgr)
                    self.save_image_index = getattr(self, 'save_image_index', 0) + 1
                except Exception:
                    pass
        except Exception:
            pass

    def _prompt_save_image_dir(self):
        try:
            default_dir = os.path.join(os.path.expanduser("~"), "Detections")
            if not os.path.isdir(default_dir):
                try:
                    os.makedirs(default_dir, exist_ok=True)
                except Exception:
                    default_dir = os.path.expanduser("~")

            folder = QFileDialog.getExistingDirectory(self, "é€‰æ‹©ä¿å­˜å›¾ç‰‡åºåˆ—çš„æ–‡ä»¶å¤¹", default_dir)
            if folder:
                self.save_image_dir = folder
                self.save_image_index = 0
                self.console.append(f"ğŸ’¾ ä¿å­˜å›¾ç‰‡åºåˆ—åˆ°: {folder}")
            else:
                self._clear_save_state()
        except Exception as e:
            self.console.append(f"âŒ é€‰æ‹©ä¿å­˜è·¯å¾„å¤±è´¥: {str(e)}")
            self._clear_save_state()

    def on_select_weight_clicked(self):
        """ç‚¹å‡»â€˜é€‰æ‹©æƒé‡â€™ï¼šç›´æ¥å¼¹å‡ºæ–‡ä»¶é€‰æ‹©ï¼Œä¸å†å¼¹æ¨¡å‹ç±»å‹èœå•ã€‚"""
        try:
            # èµ·å§‹ç›®å½•ï¼šä¼˜å…ˆæœ€è¿‘ä¸€æ¬¡æ‰€é€‰ç›®å½•ï¼›å¦åˆ™ä½¿ç”¨é¡¹ç›®æ ¹æˆ–ç”¨æˆ·ç›®å½•
            start_dir = None
            try:
                if getattr(self, 'selected_weight_path', None):
                    start_dir = os.path.dirname(self.selected_weight_path)
            except Exception:
                start_dir = None
            if not start_dir or not os.path.isdir(start_dir):
                base_dir = self._get_base_dir()
                candidate_dirs = [
                    os.path.join(base_dir, 'YOLOv8'),
                    os.path.join(base_dir, 'YOLOv5'),
                    os.path.join(base_dir, 'YOLOv3'),
                    base_dir,
                ]
                start_dir = next((d for d in candidate_dirs if os.path.isdir(d)), os.path.expanduser("~"))

            file_path, _ = QFileDialog.getOpenFileName(self, "é€‰æ‹©æƒé‡(.pt/.pth)", start_dir, "PyTorch æƒé‡ (*.pt *.pth)")
            if not file_path or not os.path.isfile(file_path):
                self.console.append("â„¹ï¸ å·²å–æ¶ˆé€‰æ‹©æƒé‡")
                return

            self.selected_weight_path = file_path
            # å–æ¶ˆå›ºå®šèœå•åï¼Œé»˜è®¤ä¸æ¨æ–­è·¨ç‰ˆæœ¬åŠ è½½ï¼›æ¸…ç©º forced_model_type
            if hasattr(self, 'forced_model_type'):
                try:
                    del self.forced_model_type
                except Exception:
                    self.forced_model_type = None
            if hasattr(self, 'lbl_weight_path'):
                self.lbl_weight_path.setText(file_path)

            self.console.append("ğŸ” åŠ è½½æ‰€é€‰æƒé‡...")
            self.model = self.load_model()
            if self.model is None:
                self.console.append("âŒ æ¨¡å‹åŠ è½½å¤±è´¥")
                return

            if self.infer_thread is not None:
                model_type_flag = getattr(self, 'model_type', 'v5')
                self.infer_thread.update_params(self.infer_size, self.frame_skip, self.infer_interval_ms, conf=getattr(self, 'confidence_value', 0.5), iou=getattr(self, 'iou_value', 0.45), model_type=model_type_flag)
            self.console.append("âœ… æƒé‡åŠ è½½å®Œæˆ")
            if hasattr(self, 'btn_play'):
                self.btn_play.setEnabled(True)
        except Exception as e:
            self.console.append(f"âŒ é€‰æ‹©æƒé‡å¤±è´¥: {str(e)}")

    def _open_video_writer(self, w: int, h: int):
        return

    def _close_video_writer(self):
        return

    def _on_infer_meta_ready(self, text: str):
        try:
            # é€æ¡è¿½åŠ æ‰€æœ‰æ£€æµ‹è®°å½•ï¼Œä¸æ¸…ç©ºå†å²
            curr = self.detection_info.toPlainText()
            new_text = (curr + "\n" + text).strip() if curr else text
            self.detection_info.setPlainText(new_text)
            # æ»šåŠ¨åˆ°æœ«å°¾
            self.detection_info.moveCursor(self.detection_info.textCursor().End)
        except Exception:
            pass