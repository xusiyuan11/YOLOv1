# -*- coding: UTF-8 -*-
"""
è‡ªåŠ¨é©¾é©¶ç›®æ ‡æ£€æµ‹ç³»ç»Ÿ - ä¸»çª—å£ç±»
"""
import sys
import os
import time
import numpy as np
from PyQt5.QtWidgets import QMainWindow, QFileDialog
from PyQt5.QtCore import Qt, QThread, pyqtSignal
from aiui import AutonomousDrivingUISetup
from aicamera import CameraVideoHandler
import torch
import cv2
from PyQt5.QtGui import QPixmap, QImage


class InferenceThread(QThread):
    """åå°æ¨ç†çº¿ç¨‹ï¼šæ¥æ”¶æœ€æ–°ä¸€å¸§ï¼ŒæŒ‰è®¾ç½®èŠ‚æµåè¿›è¡ŒYOLOv5æ¨ç†å¹¶å‘å›å åŠ åçš„QImage"""

    frame_ready = pyqtSignal(QImage)
    meta_ready = pyqtSignal(str)

    def __init__(self, model, infer_size: int, frame_skip: int, interval_ms: int):
        super().__init__()
        self.model = model
        self.infer_size = infer_size
        self.frame_skip = max(1, int(frame_skip))
        self.interval_ms = max(0, int(interval_ms))
        self._running = True
        self._busy = False
        self._last_infer_ms = 0
        self._frame_index = 0
        self._latest_frame = None  # BGR np.ndarray

    def update_params(self, infer_size: int = None, frame_skip: int = None, interval_ms: int = None):
        if infer_size is not None:
            self.infer_size = int(infer_size)
        if frame_skip is not None:
            self.frame_skip = max(1, int(frame_skip))
        if interval_ms is not None:
            self.interval_ms = max(0, int(interval_ms))

    def submit_frame(self, frame_bgr):
        # ç¡®ä¿æŒæœ‰ç‹¬ç«‹å‰¯æœ¬ï¼Œé¿å…å¼•ç”¨ä¸´æ—¶ç¼“å†²å¯¼è‡´å´©æºƒ
        try:
            self._latest_frame = frame_bgr.copy()
        except Exception:
            self._latest_frame = frame_bgr

    def stop(self):
        self._running = False

    def run(self):
        while self._running:
            try:
                frame = self._latest_frame
                if frame is None:
                    self.msleep(5)
                    continue

                # å¸§é‡‡æ ·
                self._frame_index += 1
                if (self._frame_index % self.frame_skip) != 0:
                    self.msleep(1)
                    continue

                now_ms = int(time.time() * 1000)
                if self._busy or (now_ms - self._last_infer_ms < self.interval_ms):
                    self.msleep(1)
                    continue

                self._busy = True
                try:
                    with torch.no_grad():
                        results = self.model(frame, size=self.infer_size)
                    annotated_bgr = results.render()[0]
                    annotated_rgb = cv2.cvtColor(annotated_bgr, cv2.COLOR_BGR2RGB)
                    h, w, ch = annotated_rgb.shape
                    # æ„é€ ç‹¬ç«‹QImageï¼Œé˜²æ­¢æŒ‡é’ˆæ‚¬æŒ‚
                    qt_img = QImage(annotated_rgb.copy().data, w, h, ch * w, QImage.Format_RGB888).copy()
                    self.frame_ready.emit(qt_img)
                    # ç”Ÿæˆæ£€æµ‹æ–‡æœ¬ï¼ˆç±»åˆ«/æ—¶é—´/ä½ç½®/ç½®ä¿¡åº¦ï¼‰
                    try:
                        pred = results.xyxy[0]
                        names = getattr(results, 'names', getattr(self.model, 'names', {}))
                        if pred is not None and pred.shape[0] > 0:
                            ts_ms = int(time.time() * 1000)
                            ts = time.strftime("%H:%M:%S", time.localtime(ts_ms / 1000)) + f".{ts_ms % 1000:03d}"
                            lines = []
                            for i in range(int(pred.shape[0])):
                                x1, y1, x2, y2 = [int(v) for v in pred[i, :4].detach().cpu().numpy().tolist()]
                                conf = float(pred[i, 4])
                                cls = int(pred[i, 5])
                                label = names[cls] if isinstance(names, (list, dict)) else f'class{cls}'
                                lines.append(
                                    f"{i + 1}. æ—¶é—´ {ts} | ç±»åˆ« {label} | ä½ç½® ({x1},{y1},{x2},{y2}) | ç½®ä¿¡åº¦ {conf:.2f}"
                                )
                            self.meta_ready.emit("\n".join(lines))
                        # æ— æ£€æµ‹æ—¶ä¸è¾“å‡º
                    except Exception:
                        pass
                    self._last_infer_ms = now_ms
                finally:
                    self._busy = False
            except Exception:
                # å‡ºé”™æ—¶çŸ­æš‚ä¼‘çœ ï¼Œé¿å…busy loop
                self.msleep(5)


class AutonomousDrivingUI(QMainWindow, AutonomousDrivingUISetup, CameraVideoHandler):
    """ç›®æ ‡æ£€æµ‹ç³»ç»Ÿä¸»çª—å£"""

    def __init__(self):
        super().__init__()
        # åˆå§‹åŒ–UI
        self.setup_ui()
        # åˆå§‹åŒ–æ‘„åƒå¤´å’Œè§†é¢‘å¤„ç†
        self.setup_camera_video()

        # æ£€æµ‹å¼€å…³ï¼ˆå¼€å§‹/åœæ­¢ï¼‰
        self.detecting = False
        # æ¨ç†èŠ‚æµä¸åˆ†è¾¨ç‡
        self.infer_interval_ms = 150  # æœ€å°æ¨ç†é—´éš”ï¼Œé˜²æ­¢é¢‘ç¹å¡é¡¿
        self.last_infer_ts = 0
        self.infer_size = 512  # CPU é»˜è®¤ä½¿ç”¨è¾ƒä½åˆ†è¾¨ç‡ï¼ŒGPU ä¼šåœ¨åŠ è½½åè‡ªåŠ¨æå‡
        self._infer_busy = False
        # æŒ‰å¸§æ£€æµ‹ï¼ˆç¡®ä¿è¿ç»­å¤šå¸§åŠ¨æ€æ£€æµ‹ï¼‰
        self.frame_skip = 2  # CPU é»˜è®¤æ¯2å¸§æ£€æµ‹ä¸€æ¬¡ï¼›GPUä¸‹ä¼šæ”¹ä¸ºæ¯å¸§
        self.video_frame_index = 0
        self.camera_frame_index = 0
        # åå°æ¨ç†çº¿ç¨‹
        self.infer_thread = None
        # æœ€è¿‘ä¸€æ¬¡å åŠ åçš„ç»“æœå¸§ï¼ˆç”¨äºæŒç»­æ˜¾ç¤ºï¼Œé¿å…é—ªçƒï¼‰
        self._last_annotated_qimage = None
        # UI åˆ·æ–°èŠ‚æµï¼ˆæŒ‰ç›®æ ‡å¸§ç‡é™åˆ¶æ ‡ç­¾åˆ·æ–°é¢‘ç‡ï¼‰
        self._last_ui_update_ms = 0
        self.ui_min_update_interval_ms = 1000 // 30

        # åŠ è½½YOLOv5æ¨¡å‹
        self.model = self.load_model()

        # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
        self.recent_detections = []  # å­˜å‚¨æœ€è¿‘æ£€æµ‹ç»“æœ
        self.reset_stats()

    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.update_detection_info(0, 0, 0, 0, {})

    def update_detection_info(self, targets, inference_time, fps, progress, class_counts, status="å¾…æœºä¸­"):
        """ä¸å†æ„å»ºå›ºå®šä¿¡æ¯æ–‡æœ¬ï¼Œä¿æŒä¸ç”¨æˆ·è¦æ±‚ä¸€è‡´ï¼Œç”±æ¨ç†çº¿ç¨‹è¿½åŠ æ—¥å¿—"""
        return

    def get_recent_detections(self):
        """è·å–æœ€è¿‘æ£€æµ‹ç»“æœ"""
        if hasattr(self, 'recent_detections') and self.recent_detections:
            return "\n   ".join(self.recent_detections[-3:])  # æ˜¾ç¤ºæœ€è¿‘3æ¡
        return "æš‚æ— æ£€æµ‹æ•°æ®"

    def get_system_tip(self, status):
        """è·å–ç³»ç»Ÿæç¤º"""
        tips = {
            "å¾…æœºä¸­": "è¯·é€‰æ‹©è¾“å…¥æºå¼€å§‹æ£€æµ‹",
            "æ£€æµ‹ä¸­": "æ­£åœ¨å¤„ç†è¾“å…¥æ•°æ®ï¼Œè¯·ç¨å€™...",
            "å®Œæˆ": "æ£€æµ‹å®Œæˆï¼Œå¯æŸ¥çœ‹ç»“æœ",
            "é”™è¯¯": "æ£€æµ‹è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œè¯·æ£€æŸ¥è¾“å…¥"
        }
        return tips.get(status, "ç³»ç»Ÿè¿è¡Œæ­£å¸¸")

    def connect_signals(self):
        """è¿æ¥æ‰€æœ‰ä¿¡å·å’Œæ§½å‡½æ•°"""
        # å‚æ•°æ§åˆ¶ä¿¡å· - ç°åœ¨é€šè¿‡æŒ‰é’®å¼¹çª—å¤„ç†ï¼Œä¸éœ€è¦ç›´æ¥è¿æ¥
        # self.conf_spin.valueChanged.connect(self.on_conf_changed)
        # self.conf_slider.valueChanged.connect(self.on_conf_slider_changed)
        # self.iou_spin.valueChanged.connect(self.on_iou_changed)
        # self.iou_slider.valueChanged.connect(self.on_iou_slider_changed)

        # è¾“å…¥æºé€‰æ‹©ä¿¡å·
        self.btn_image.clicked.connect(self.on_image_clicked)
        self.btn_video.clicked.connect(self.on_video_clicked)
        self.btn_camera.clicked.connect(self.on_camera_clicked)

        # æ§åˆ¶ä¿¡å·
        self.btn_play.clicked.connect(self.on_play_clicked)
        self.btn_stop.clicked.connect(self.on_stop_clicked)

    # å‚æ•°è°ƒæ•´å›è°ƒå‡½æ•°
    def on_conf_changed(self, value):
        """Confidenceå€¼æ”¹å˜"""
        self.conf_slider.setValue(int(value * 100))
        self.console.append(f"ğŸ¯ Confidenceé˜ˆå€¼è°ƒæ•´ä¸º: {value:.2f}")

    def on_conf_slider_changed(self, value):
        """Confidenceæ»‘å—æ”¹å˜"""
        self.conf_spin.setValue(value / 100.0)

    def on_iou_changed(self, value):
        """IOUå€¼æ”¹å˜"""
        self.iou_slider.setValue(int(value * 100))
        self.console.append(f"ğŸ“ IOUé˜ˆå€¼è°ƒæ•´ä¸º: {value:.2f}")

    def on_iou_slider_changed(self, value):
        """IOUæ»‘å—æ”¹å˜"""
        self.iou_spin.setValue(value / 100.0)

    def load_model(self):
        """åŠ è½½YOLOv5æ¨¡å‹ï¼ˆä¼˜å…ˆæœ¬åœ°ä»“åº“ï¼‰ï¼Œæ”¯æŒè‡ªå®šä¹‰æƒé‡"""
        # æ¨æµ‹æœ¬åœ°YOLOv5ä»“åº“ä½ç½®ï¼š.../YOLOv5/yolov5-master
        base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))
        default_repo_paths = [
            os.path.join(base_dir, 'YOLOv5', 'yolov5-master'),
            os.path.join(base_dir, 'yolov5'),
        ]

        yolo_repo = next((p for p in default_repo_paths if os.path.isdir(p)), None)
        # é¿å… torch.hub ç¼“å­˜å¯¼è‡´åŠ è½½æ—§ä»£ç ï¼Œå¼ºåˆ¶æœ¬åœ°ä¼˜å…ˆæ—¶å¯å…³é—­ç¼“å­˜
        try:
            torch.hub.set_dir(os.path.join(base_dir, '.torchhub'))
        except Exception:
            pass
        if yolo_repo is None:
            self.console.append("âš ï¸ æœªæ‰¾åˆ°æœ¬åœ°YOLOv5ä»“åº“ï¼Œå°†å°è¯•è”ç½‘åŠ è½½ï¼ˆå¯èƒ½è¾ƒæ…¢ï¼‰")

        # å¯èƒ½çš„é»˜è®¤æƒé‡æ–‡ä»¶å€™é€‰ï¼ˆä¼˜å…ˆä½¿ç”¨ç”¨æˆ·æä¾›çš„ yolov5x.ptï¼‰
        default_weight_candidates = [
            os.path.join(base_dir, 'YOLOv5', 'yolov5-master', 'yolov5x.pt'),
            os.path.join(base_dir, 'best.pt'),
            os.path.join(base_dir, 'weights', 'best.pt'),
            os.path.join(base_dir, 'yolov5s.pt'),
            os.path.join(base_dir, 'YOLOv5', 'yolov5-master', 'yolov5s.pt'),
        ]
        weights_path = next((w for w in default_weight_candidates if os.path.isfile(w)), None)

        if weights_path is None:
            # è®©ç”¨æˆ·é€‰æ‹©æƒé‡
            self.console.append("ğŸ“‚ è¯·é€‰æ‹©YOLOv5æƒé‡æ–‡ä»¶ (*.pt)")
            file_path, _ = QFileDialog.getOpenFileName(
                self, "é€‰æ‹©YOLOv5æƒé‡æ–‡ä»¶", base_dir, "PyTorch æƒé‡ (*.pt)"
            )
            weights_path = file_path if file_path else None

        try:
            self.console.append(f"ğŸ§© Torch ç‰ˆæœ¬: {getattr(torch, '__version__', 'unknown')}")
            self.console.append(f"ğŸ“ ä»“åº“è·¯å¾„: {yolo_repo if yolo_repo else 'remote: ultralytics/yolov5'}")
            self.console.append(f"ğŸ“¦ æƒé‡è·¯å¾„: {weights_path if weights_path else 'é¢„è®­ç»ƒ yolov5s'}")

            if weights_path:
                if yolo_repo and os.path.isdir(yolo_repo):
                    try:
                        model = torch.hub.load(yolo_repo, 'custom', path=weights_path, source='local', trust_repo=True)
                    except TypeError:
                        model = torch.hub.load(yolo_repo, 'custom', path=weights_path, source='local')
                else:
                    try:
                        model = torch.hub.load('ultralytics/yolov5', 'custom', path=weights_path, trust_repo=True)
                    except TypeError:
                        model = torch.hub.load('ultralytics/yolov5', 'custom', path=weights_path)
                self.console.append(f"âœ… YOLOv5 æ¨¡å‹åŠ è½½æˆåŠŸ: {os.path.basename(weights_path)}")
            else:
                # æ— è‡ªå®šä¹‰æƒé‡ï¼Œå›é€€åˆ°yolov5s
                if yolo_repo and os.path.isdir(yolo_repo):
                    try:
                        model = torch.hub.load(yolo_repo, 'yolov5s', source='local', trust_repo=True)
                    except TypeError:
                        model = torch.hub.load(yolo_repo, 'yolov5s', source='local')
                else:
                    try:
                        model = torch.hub.load('ultralytics/yolov5', 'yolov5s', trust_repo=True)
                    except TypeError:
                        model = torch.hub.load('ultralytics/yolov5', 'yolov5s')
                self.console.append("âœ… å·²åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ yolov5s")

            # è®¾ç½®é˜ˆå€¼ï¼ˆä¸UIä¿æŒä¸€è‡´ï¼‰ï¼Œå…¼å®¹ä¸åŒç‰ˆæœ¬API
            try:
                if hasattr(self, 'confidence_value') and hasattr(model, 'conf'):
                    model.conf = float(self.confidence_value)
                if hasattr(self, 'iou_value') and hasattr(model, 'iou'):
                    model.iou = float(self.iou_value)
            except Exception:
                pass

            # æ¨ç†è®¾å¤‡
            try:
                device = 'cuda' if torch.cuda.is_available() else 'cpu'
                model.to(device)
                self.console.append(f"ğŸ–¥ï¸ æ¨ç†è®¾å¤‡: {device}")
                # GPU ç”¨é«˜åˆ†è¾¨ç‡ï¼ŒCPU ç”¨è¾ƒä½åˆ†è¾¨ç‡
                self.infer_size = 640 if device == 'cuda' else 512
                self.frame_skip = 1 if device == 'cuda' else 2
            except Exception:
                pass

            # æ¨¡å‹é¢„çƒ­ï¼šé¿å…é¦–å¸§æ¨ç†å¡é¡¿
            try:
                import numpy as _np
                dummy = _np.zeros((self.infer_size, self.infer_size, 3), dtype=_np.uint8)
                with torch.no_grad():
                    _ = model(dummy, size=self.infer_size)
            except Exception:
                pass

            # æ›´æ–°æ¨¡å‹çŠ¶æ€æ ‡ç­¾
            if hasattr(self, 'lbl_model_status'):
                self.lbl_model_status.setText("ğŸ“‹ æ¨¡å‹å°±ç»ª (YOLOv5)")

            return model
        except Exception as e:
            self.console.append(f"âŒ YOLOv5 æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            return None

    def detect_image(self, image_path):
        """ä½¿ç”¨YOLOv5æ£€æµ‹å›¾åƒå¹¶æ˜¾ç¤ºæ ‡æ³¨ç»“æœï¼ˆæ”¯æŒä¸­æ–‡è·¯å¾„ï¼‰"""
        try:
            if self.model is None:
                self.console.append("âš ï¸ æœªåŠ è½½æ¨¡å‹ï¼Œæ— æ³•è¿›è¡Œæ£€æµ‹")
                return

            # è¯»å–åŸå§‹å›¾åƒï¼ˆBGRï¼‰ï¼Œæ”¯æŒä¸­æ–‡è·¯å¾„
            bgr = cv2.imdecode(np.fromfile(image_path, dtype=np.uint8), cv2.IMREAD_COLOR)
            if bgr is None:
                self.console.append(f"âŒ æ— æ³•è¯»å–å›¾åƒæ–‡ä»¶: {image_path}")
                return

            start_ts = time.time()
            # è®¾ç½®é˜ˆå€¼çƒ­æ›´æ–°ï¼ˆé˜²æ­¢ç”¨æˆ·è°ƒæ•´åæœªç”Ÿæ•ˆï¼‰
            try:
                if hasattr(self, 'confidence_value') and hasattr(self.model, 'conf'):
                    self.model.conf = float(self.confidence_value)
                if hasattr(self, 'iou_value') and hasattr(self.model, 'iou'):
                    self.model.iou = float(self.iou_value)
            except Exception:
                pass

            # æ¨ç†ï¼ˆèŠ‚æµä¸éœ€è¦ï¼Œå› ä¸ºå•æ¬¡å›¾åƒæ£€æµ‹ï¼‰
            with torch.no_grad():
                results = self.model(bgr, size=self.infer_size)
            inference_ms = int((time.time() - start_ts) * 1000)

            # æ¸²æŸ“æ ‡æ³¨ï¼ˆBGRï¼‰
            rendered = results.render()[0]

            # æ˜¾ç¤ºåˆ°UIï¼ˆè½¬RGBå†æ˜¾ç¤ºï¼‰
            rgb = cv2.cvtColor(rendered, cv2.COLOR_BGR2RGB)
            h, w, ch = rgb.shape
            qt_img = QImage(rgb.data, w, h, ch * w, QImage.Format_RGB888)
            pixmap = QPixmap.fromImage(qt_img)
            scaled_pixmap = pixmap.scaled(self.lbl_video.size(), Qt.KeepAspectRatio, Qt.SmoothTransformation)
            self.lbl_video.setPixmap(scaled_pixmap)

            # è§£æç»“æœå¹¶æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            targets, class_counts, recent_text = self._parse_yolo_results(results)
            self.recent_detections.append(recent_text)
            self.update_detection_info(targets, inference_ms, 0, 100, class_counts, status="å®Œæˆ")

            # è‡ªåŠ¨ä¿å­˜ç»“æœ
            if hasattr(self, 'cb_save') and self.cb_save.isChecked():
                save_path = os.path.splitext(image_path)[0] + "_det.jpg"
                cv2.imencode('.jpg', rendered)[1].tofile(save_path)
                self.console.append(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {os.path.basename(save_path)}")

            self.console.append("âœ… å›¾åƒæ£€æµ‹å®Œæˆ")

        except Exception as e:
            self.console.append(f"âŒ å›¾åƒæ£€æµ‹å¤±è´¥: {str(e)}")

    def _parse_yolo_results(self, results):
        """ä»YOLOv5ç»“æœä¸­ç»Ÿè®¡ç›®æ ‡æ•°ä¸ç±»åˆ«åˆ†å¸ƒï¼Œè¿”å›(æ€»æ•°, ç±»åˆ«ç»Ÿè®¡, æœ€è¿‘ç»“æœæ–‡æœ¬)"""
        try:
            # è½»é‡è§£æï¼Œé¿å…é¦–å¸§å¼•å…¥ pandas çš„å¼€é”€
            pred = results.xyxy[0]  # tensor: [N, 6] -> x1,y1,x2,y2,conf,cls
            total = int(pred.shape[0]) if pred is not None else 0

            name_map = {
                'car': 'è½¦è¾†',
                'person': 'è¡Œäºº',
                'traffic light': 'äº¤é€šç¯',
                'stop sign': 'æ ‡å¿—ç‰Œ',
                'bus': 'å…¬äº¤è½¦',
                'truck': 'å¡è½¦',
                'motorcycle': 'æ‘©æ‰˜è½¦',
                'bicycle': 'è‡ªè¡Œè½¦',
            }

            class_counts = {}
            recent_text = "æ— æ£€æµ‹"
            if total > 0:
                conf = pred[:, 4].detach().cpu().numpy()
                cls = pred[:, 5].detach().cpu().numpy().astype(int)
                names = getattr(results, 'names', getattr(self.model, 'names', {}))

                import numpy as _np
                unique_cls = _np.unique(cls)
                for ci in unique_cls:
                    en = names[int(ci)] if isinstance(names, (list, dict)) else f'class{int(ci)}'
                    cn = name_map.get(en, en)
                    class_counts[cn] = int((_np.array(cls) == ci).sum())

                # å–ç½®ä¿¡åº¦æœ€é«˜çš„å‰3ä¸ª
                top_idx = _np.argsort(-conf)[:3]
                labels = []
                for i in top_idx:
                    en = names[int(cls[i])] if isinstance(names, (list, dict)) else f'class{int(cls[i])}'
                    cn = name_map.get(en, en)
                    labels.append(f"{cn} {conf[i]:.2f}")
                if labels:
                    recent_text = ", ".join(labels)

            return total, class_counts, recent_text
        except Exception:
            return 0, {}, "è§£æå¤±è´¥"

    # è¦†ç›–æ‘„åƒå¤´å¸§æ˜¾ç¤ºä»¥åŠ å…¥å®æ—¶æ£€æµ‹ï¼ˆå¯ç”±å¼€å§‹/åœæ­¢æŒ‰é’®æ§åˆ¶ï¼‰
    def show_real_video(self, h, w, c, data):
        try:
            frame_bgr = np.frombuffer(data, dtype=np.uint8).reshape((h, w, c))
            # å°†å¸§é€å…¥åå°çº¿ç¨‹è¿›è¡Œæ¨ç†
            if self.detecting and self.model is not None and self.infer_thread is not None:
                self.infer_thread.submit_frame(frame_bgr)

            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)
            raw_qimage = QImage(frame_rgb.data, w, h, c * w, QImage.Format_RGB888)

            # æ£€æµ‹ä¸­ä¼˜å…ˆæ˜¾ç¤ºæœ€è¿‘çš„å åŠ ç»“æœï¼Œé¿å…åŸå§‹å¸§è¦†ç›–é€ æˆâ€œé—ªçƒâ€ï¼›å¹¶é™åˆ¶åˆ·æ–°é¢‘ç‡
            now_ms = int(time.time() * 1000)
            if now_ms - self._last_ui_update_ms >= self.ui_min_update_interval_ms:
                if self.detecting and self._last_annotated_qimage is not None:
                    pixmap = QPixmap.fromImage(self._last_annotated_qimage)
                else:
                    pixmap = QPixmap.fromImage(raw_qimage)
                scaled_pixmap = pixmap.scaled(self.lbl_video.size(), Qt.KeepAspectRatio, Qt.FastTransformation)
                self.lbl_video.setPixmap(scaled_pixmap)
                self._last_ui_update_ms = now_ms
        except Exception as e:
            print(f"æ˜¾ç¤ºæ‘„åƒå¤´ç”»é¢é”™è¯¯: {e}")

    def display_video_frame(self, qt_image):
        """è§†é¢‘æ’­æ”¾å¸§å›è°ƒï¼šåœ¨å®æ—¶æ£€æµ‹å¼€å¯æ—¶æ‰§è¡ŒYOLOv5æ¨ç†å¹¶å åŠ ç»“æœ"""
        try:
            w = qt_image.width()
            h = qt_image.height()
            ch = 3

            # å°†å¸§é€å…¥åå°çº¿ç¨‹è¿›è¡Œæ¨ç†
            if self.detecting and self.model is not None and self.infer_thread is not None:
                bits = qt_image.bits()
                bits.setsize(h * w * ch)
                rgb = np.frombuffer(bits, dtype=np.uint8).reshape((h, w, ch))
                bgr = cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)
                self.infer_thread.submit_frame(bgr)

            # æ£€æµ‹ä¸­ä¼˜å…ˆæ˜¾ç¤ºæœ€è¿‘çš„å åŠ ç»“æœï¼Œé¿å…åŸå§‹å¸§è¦†ç›–é€ æˆâ€œé—ªçƒâ€ï¼›å¹¶é™åˆ¶åˆ·æ–°é¢‘ç‡
            now_ms = int(time.time() * 1000)
            if now_ms - self._last_ui_update_ms >= self.ui_min_update_interval_ms:
                if self.detecting and self._last_annotated_qimage is not None:
                    pixmap = QPixmap.fromImage(self._last_annotated_qimage)
                else:
                    pixmap = QPixmap.fromImage(qt_image)
                scaled_pixmap = pixmap.scaled(self.lbl_video.size(), Qt.KeepAspectRatio, Qt.FastTransformation)
                self.lbl_video.setPixmap(scaled_pixmap)
                self._last_ui_update_ms = now_ms
        except Exception as e:
            print(f"æ˜¾ç¤ºè§†é¢‘å¸§é”™è¯¯: {e}")

    # è¾“å…¥æºé€‰æ‹©å›è°ƒå‡½æ•°
    def on_image_clicked(self):
        """å›¾åƒæ£€æµ‹æŒ‰é’®ç‚¹å‡»"""
        file_path, _ = QFileDialog.getOpenFileName(
            self, "é€‰æ‹©å›¾åƒæ–‡ä»¶", "", "å›¾åƒæ–‡ä»¶ (*.png *.jpg *.jpeg *.bmp)"
        )
        if file_path:
            self.stop_all()  # åœæ­¢å…¶ä»–æ¨¡å¼
            self.current_image_path = file_path
            self.console.append(f"ğŸ–¼ï¸ å·²é€‰æ‹©å›¾åƒ: {os.path.basename(file_path)}")
            # æ˜¾ç¤ºå›¾åƒå¹¶è¿›è¡Œæ£€æµ‹
            self.detect_image(file_path)

    def on_video_clicked(self):
        """è§†é¢‘æ£€æµ‹æŒ‰é’®ç‚¹å‡»"""
        file_path, _ = QFileDialog.getOpenFileName(
            self, "é€‰æ‹©è§†é¢‘æ–‡ä»¶", "", "è§†é¢‘æ–‡ä»¶ (*.mp4 *.avi *.mov *.mkv)"
        )
        if file_path:
            self.stop_all()  # åœæ­¢å…¶ä»–æ¨¡å¼
            self.console.append(f"ğŸ¬ å·²é€‰æ‹©è§†é¢‘: {os.path.basename(file_path)}")
            self.start_video_playback(file_path)
            # å¼€å§‹åå°æ¨ç†çº¿ç¨‹
            self._start_infer_thread()

    def on_camera_clicked(self):
        """æ‘„åƒå¤´æŒ‰é’®ç‚¹å‡»"""
        if not self.camera_running:
            # åœæ­¢å…¶ä»–æ¨¡å¼
            self.stop_video_playback()

            # å°è¯•å¯åŠ¨çœŸå®æ‘„åƒå¤´
            if self.start_camera():
                self.console.append("ğŸ“¹ å¯åŠ¨çœŸå®æ‘„åƒå¤´æ£€æµ‹")
                self.btn_camera.setText("ğŸ“¹ åœæ­¢æ‘„åƒå¤´")
                self.btn_camera.setStyleSheet("""
                    QPushButton {
                        background: qlineargradient(x1: 0, y1: 0, x2: 0, y2: 1,
                            stop: 0 #e74c3c, stop: 1 #c0392b);
                        font-size: 13px;
                    }
                """)
                # é‡ç½®ç»Ÿè®¡ä¿¡æ¯
                self.reset_stats()
            else:
                self.console.append("âŒ æ— æ³•å¯åŠ¨æ‘„åƒå¤´")
        else:
            # åœæ­¢æ‘„åƒå¤´
            self.stop_camera()
            self.btn_camera.setText("ğŸ“¹ å®æ—¶æ‘„åƒå¤´")
            self.btn_camera.setStyleSheet("""
                QPushButton {
                    background: qlineargradient(x1: 0, y1: 0, x2: 0, y2: 1,
                        stop: 0 #8e44ad, stop: 1 #6c3483);
                    font-size: 13px;
                }
            """)

    # æ§åˆ¶å›è°ƒå‡½æ•°
    def on_play_clicked(self):
        """å¼€å§‹æ£€æµ‹æŒ‰é’®ç‚¹å‡»"""
        # å¼€å¯æ£€æµ‹æ¨¡å¼ï¼ˆç”¨äºæ‘„åƒå¤´/è§†é¢‘ï¼‰
        self.detecting = True
        self._start_infer_thread()
        # å¦‚å‹¾é€‰ä¿å­˜ç»“æœï¼Œè¯¢é—®ä¿å­˜è·¯å¾„ï¼ˆè§†é¢‘/æ‘„åƒå¤´ï¼‰
        if hasattr(self, 'cb_save') and self.cb_save.isChecked():
            if (self.camera_running or (self.video_thread and self.video_thread.isRunning())):
                self._prompt_save_video_path()
        if self.camera_running:
            self.console.append("â–¶ï¸ å·²å¯ç”¨æ‘„åƒå¤´å®æ—¶æ£€æµ‹")
        elif self.video_thread and self.video_thread.isRunning():
            self.console.append("â–¶ï¸ å·²å¯ç”¨è§†é¢‘å®æ—¶æ£€æµ‹")
        elif self.current_image_path:
            self.console.append("â–¶ï¸ å¼€å§‹å›¾åƒæ£€æµ‹")
            self.detect_image(self.current_image_path)
        else:
            self.console.append("âš ï¸ è¯·å…ˆé€‰æ‹©è¾“å…¥æº")

    def on_stop_clicked(self):
        """åœæ­¢æ£€æµ‹æŒ‰é’®ç‚¹å‡»"""
        # å…³é—­å®æ—¶æ£€æµ‹
        self.detecting = False
        self.stop_all()
        self._stop_infer_thread()
        self.console.append("â¹ï¸ åœæ­¢æ‰€æœ‰æ£€æµ‹")
        # é‡ç½®ç»Ÿè®¡ä¿¡æ¯
        self.reset_stats()
        # åœæ­¢åä¸å†ä¿ç•™å†å²è¾“å‡ºï¼ˆæ ¹æ®éœ€è¦å¯æ³¨é‡Šæ‰ï¼‰
        self.detection_info.setPlainText("")
        # æ¸…ç©ºå åŠ å¸§ç¼“å­˜
        self._last_annotated_qimage = None
        # å…³é—­è§†é¢‘å†™å…¥
        self._close_video_writer()

    def closeEvent(self, event):
        """çª—å£å…³é—­äº‹ä»¶"""
        self.stop_all()
        self._stop_infer_thread()
        self._last_annotated_qimage = None
        event.accept()

    def _start_infer_thread(self):
        """å¯åŠ¨æˆ–æ›´æ–°åå°æ¨ç†çº¿ç¨‹å‚æ•°"""
        if self.model is None:
            return
        if self.infer_thread is None:
            self.infer_thread = InferenceThread(self.model, self.infer_size, self.frame_skip, self.infer_interval_ms)
            self.infer_thread.frame_ready.connect(self._on_infer_frame_ready)
            self.infer_thread.meta_ready.connect(self._on_infer_meta_ready)
            # è®¾ä¸ºå®ˆæŠ¤çº¿ç¨‹ï¼Œé¿å…è¿›ç¨‹é€€å‡ºé˜»å¡
            try:
                self.infer_thread.setObjectName("InferenceThread")
            except Exception:
                pass
            self.infer_thread.start()
        else:
            self.infer_thread.update_params(self.infer_size, self.frame_skip, self.infer_interval_ms)

    def _stop_infer_thread(self):
        if self.infer_thread is not None:
            try:
                self.infer_thread.stop()
                self.infer_thread.wait(2000)
            except Exception:
                pass
            self.infer_thread = None

    def _on_infer_frame_ready(self, qt_image: QImage):
        """æ”¶åˆ°åå°æ¨ç†ç»“æœå¸§ï¼Œéé˜»å¡æ›´æ–°UI"""
        try:
            # ç¼“å­˜æœ€æ–°çš„å åŠ ç»“æœï¼Œä¾›åŸå§‹å¸§åˆ°è¾¾æ—¶ä¹Ÿä¼˜å…ˆæ˜¾ç¤º
            self._last_annotated_qimage = qt_image
            now_ms = int(time.time() * 1000)
            if now_ms - self._last_ui_update_ms >= self.ui_min_update_interval_ms:
                pixmap = QPixmap.fromImage(qt_image)
                scaled_pixmap = pixmap.scaled(self.lbl_video.size(), Qt.KeepAspectRatio, Qt.FastTransformation)
                self.lbl_video.setPixmap(scaled_pixmap)
                self._last_ui_update_ms = now_ms

            # å†™å…¥è§†é¢‘ç»“æœ
            if getattr(self, 'saving_video', False) and getattr(self, 'save_video_path', None):
                w = qt_image.width()
                h = qt_image.height()
                ch = 3
                bits = qt_image.bits()
                bits.setsize(h * w * ch)
                rgb = np.frombuffer(bits, dtype=np.uint8).reshape((h, w, ch))
                bgr = cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)
                if getattr(self, 'video_writer', None) is None or getattr(self, '_video_writer_size', None) != (w, h):
                    self._open_video_writer(w, h)
                if getattr(self, 'video_writer', None) is not None:
                    self.video_writer.write(bgr)
        except Exception:
            pass

    def _prompt_save_video_path(self):
        try:
            default_name = time.strftime("%Y%m%d_%H%M%S") + ".mp4"
            path, _ = QFileDialog.getSaveFileName(self, "ä¿å­˜æ£€æµ‹è§†é¢‘ä¸º", default_name, "è§†é¢‘æ–‡ä»¶ (*.mp4 *.avi)")
            if path:
                self.save_video_path = path
                self.saving_video = True
                self.console.append(f"ğŸ’¾ ä¿å­˜è·¯å¾„: {os.path.basename(path)}")
            else:
                self.saving_video = False
                self.save_video_path = None
        except Exception as e:
            self.console.append(f"âŒ é€‰æ‹©ä¿å­˜è·¯å¾„å¤±è´¥: {str(e)}")
            self.saving_video = False
            self.save_video_path = None

    def _open_video_writer(self, w: int, h: int):
        try:
            if not getattr(self, 'saving_video', False) or not getattr(self, 'save_video_path', None):
                return
            # å…³é—­æ—§çš„
            self._close_video_writer()
            # fourcc æ ¹æ®æ‰©å±•åé€‰æ‹©
            ext = os.path.splitext(self.save_video_path)[1].lower()
            if ext == ".avi":
                fourcc = cv2.VideoWriter_fourcc(*"XVID")
            else:
                fourcc = cv2.VideoWriter_fourcc(*"mp4v")
            fps = int(getattr(self, 'target_fps', 20))
            self.video_writer = cv2.VideoWriter(self.save_video_path, fourcc, max(1, fps), (w, h))
            self._video_writer_size = (w, h)
            if not self.video_writer.isOpened():
                self.console.append("âŒ æ— æ³•åˆ›å»ºè§†é¢‘å†™å…¥å™¨")
                self.video_writer = None
        except Exception as e:
            self.console.append(f"âŒ åˆ›å»ºè§†é¢‘å†™å…¥å™¨å¤±è´¥: {str(e)}")
            self.video_writer = None

    def _close_video_writer(self):
        try:
            if getattr(self, 'video_writer', None) is not None:
                self.video_writer.release()
            if getattr(self, 'save_video_path', None) and getattr(self, 'saving_video', False):
                self.console.append(f"âœ… å·²ä¿å­˜è§†é¢‘: {os.path.basename(self.save_video_path)}")
        except Exception:
            pass
        finally:
            self.video_writer = None
            self._video_writer_size = None
            self.saving_video = False

    def _on_infer_meta_ready(self, text: str):
        try:
            # é€æ¡è¿½åŠ æ‰€æœ‰æ£€æµ‹è®°å½•ï¼Œä¸æ¸…ç©ºå†å²
            curr = self.detection_info.toPlainText()
            new_text = (curr + "\n" + text).strip() if curr else text
            self.detection_info.setPlainText(new_text)
            # æ»šåŠ¨åˆ°æœ«å°¾
            self.detection_info.moveCursor(self.detection_info.textCursor().End)
        except Exception:
            pass