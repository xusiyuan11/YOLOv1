# -*- coding: UTF-8 -*-
"""
è‡ªåŠ¨é©¾é©¶ç›®æ ‡æ£€æµ‹ç³»ç»Ÿ - ä¸»çª—å£ç±»
"""
import sys
import os
import time
import numpy as np
from PyQt5.QtWidgets import QMainWindow, QFileDialog, QInputDialog
from PyQt5.QtCore import Qt, QThread, pyqtSignal
from aiui import AutonomousDrivingUISetup
from aicamera import CameraVideoHandler
import torch
import cv2
from PyQt5.QtGui import QPixmap, QImage


class InferenceThread(QThread):
    """åå°æ¨ç†çº¿ç¨‹ï¼šæ¥æ”¶æœ€æ–°ä¸€å¸§ï¼ŒæŒ‰è®¾ç½®èŠ‚æµåè¿›è¡ŒYOLOæ¨ç†å¹¶å‘å›å åŠ åçš„QImageï¼ˆå…¼å®¹v5/v8ï¼‰"""

    frame_ready = pyqtSignal(QImage)
    meta_ready = pyqtSignal(str)

    def __init__(self, model, infer_size: int, frame_skip: int, interval_ms: int, model_type: str = 'v5', conf: float = 0.5, iou: float = 0.45):
        super().__init__()
        self.model = model
        self.infer_size = infer_size
        self.frame_skip = max(1, int(frame_skip))
        self.interval_ms = max(0, int(interval_ms))
        self.model_type = 'v8' if (str(model_type).lower().find('v8') != -1 or str(model_type).lower().find('8') != -1) else 'v5'
        self.confidence = float(conf)
        self.iou = float(iou)
        self._running = True
        self._busy = False
        self._last_infer_ms = 0
        self._frame_index = 0
        self._latest_frame = None  # BGR np.ndarray

    def update_params(self, infer_size: int = None, frame_skip: int = None, interval_ms: int = None, conf: float = None, iou: float = None, model_type: str = None):
        if infer_size is not None:
            self.infer_size = int(infer_size)
        if frame_skip is not None:
            self.frame_skip = max(1, int(frame_skip))
        if interval_ms is not None:
            self.interval_ms = max(0, int(interval_ms))
        if conf is not None:
            self.confidence = float(conf)
        if iou is not None:
            self.iou = float(iou)
        if model_type is not None:
            self.model_type = 'v8' if (str(model_type).lower().find('v8') != -1 or str(model_type).lower().find('8') != -1) else 'v5'

    def submit_frame(self, frame_bgr):
        # ç¡®ä¿æŒæœ‰ç‹¬ç«‹å‰¯æœ¬ï¼Œé¿å…å¼•ç”¨ä¸´æ—¶ç¼“å†²å¯¼è‡´å´©æºƒ
        try:
            self._latest_frame = frame_bgr.copy()
        except Exception:
            self._latest_frame = frame_bgr

    def stop(self):
        self._running = False

    def run(self):
        while self._running:
            try:
                frame = self._latest_frame
                if frame is None:
                    self.msleep(5)
                    continue

                # å¸§é‡‡æ ·
                self._frame_index += 1
                if (self._frame_index % self.frame_skip) != 0:
                    self.msleep(1)
                    continue

                now_ms = int(time.time() * 1000)
                if self._busy or (now_ms - self._last_infer_ms < self.interval_ms):
                    self.msleep(1)
                    continue

                self._busy = True
                try:
                    with torch.no_grad():
                        if self.model_type == 'v8':
                            # YOLOv8 æ¨ç†
                            results = self.model(frame, imgsz=int(self.infer_size), conf=float(self.confidence), iou=float(self.iou))
                            annotated_bgr = results[0].plot()
                            names = getattr(results[0], 'names', getattr(self.model, 'names', {}))
                            # å…ƒä¿¡æ¯
                            try:
                                boxes = getattr(results[0], 'boxes', None)
                                if boxes is not None and hasattr(boxes, 'xyxy') and boxes.xyxy is not None and boxes.xyxy.shape[0] > 0:
                                    ts_ms = int(time.time() * 1000)
                                    ts = time.strftime("%H:%M:%S", time.localtime(ts_ms / 1000)) + f".{ts_ms % 1000:03d}"
                                    lines = []
                                    count = boxes.xyxy.shape[0]
                                    for i in range(count):
                                        x1, y1, x2, y2 = [int(v) for v in boxes.xyxy[i].detach().cpu().numpy().tolist()]
                                        conf = float(boxes.conf[i].detach().cpu().numpy().tolist())
                                        cls = int(boxes.cls[i].detach().cpu().numpy().tolist())
                                        label = names[cls] if isinstance(names, (list, dict)) else (names[cls] if isinstance(names, list) else f'class{cls}')
                                        lines.append(f"{i + 1}. æ—¶é—´ {ts} | ç±»åˆ« {label} | ä½ç½® ({x1},{y1},{x2},{y2}) | ç½®ä¿¡åº¦ {conf:.2f}")
                                    self.meta_ready.emit("\n".join(lines))
                            except Exception:
                                pass
                        else:
                            # YOLOv5 æ¨ç†
                            results = self.model(frame, size=int(self.infer_size))
                            annotated_bgr = results.render()[0]
                            names = getattr(results, 'names', getattr(self.model, 'names', {}))
                            # å…ƒä¿¡æ¯
                            try:
                                pred = results.xyxy[0]
                                if pred is not None and pred.shape[0] > 0:
                                    ts_ms = int(time.time() * 1000)
                                    ts = time.strftime("%H:%M:%S", time.localtime(ts_ms / 1000)) + f".{ts_ms % 1000:03d}"
                                    lines = []
                                    for i in range(int(pred.shape[0])):
                                        x1, y1, x2, y2 = [int(v) for v in pred[i, :4].detach().cpu().numpy().tolist()]
                                        conf = float(pred[i, 4])
                                        cls = int(pred[i, 5])
                                        label = names[cls] if isinstance(names, (list, dict)) else f'class{cls}'
                                        lines.append(f"{i + 1}. æ—¶é—´ {ts} | ç±»åˆ« {label} | ä½ç½® ({x1},{y1},{x2},{y2}) | ç½®ä¿¡åº¦ {conf:.2f}")
                                    self.meta_ready.emit("\n".join(lines))
                            except Exception:
                                pass

                    annotated_rgb = cv2.cvtColor(annotated_bgr, cv2.COLOR_BGR2RGB)
                    h, w, ch = annotated_rgb.shape
                    # æ„é€ ç‹¬ç«‹QImageï¼Œé˜²æ­¢æŒ‡é’ˆæ‚¬æŒ‚
                    qt_img = QImage(annotated_rgb.copy().data, w, h, ch * w, QImage.Format_RGB888).copy()
                    self.frame_ready.emit(qt_img)
                    self._last_infer_ms = now_ms
                finally:
                    self._busy = False
            except Exception:
                # å‡ºé”™æ—¶çŸ­æš‚ä¼‘çœ ï¼Œé¿å…busy loop
                self.msleep(5)


class AutonomousDrivingUI(QMainWindow, AutonomousDrivingUISetup, CameraVideoHandler):
    """ç›®æ ‡æ£€æµ‹ç³»ç»Ÿä¸»çª—å£"""

    def __init__(self):
        super().__init__()
        # åˆå§‹åŒ–UI
        self.setup_ui()
        # åˆå§‹åŒ–æ‘„åƒå¤´å’Œè§†é¢‘å¤„ç†
        self.setup_camera_video()

        # æ£€æµ‹å¼€å…³ï¼ˆå¼€å§‹/åœæ­¢ï¼‰
        self.detecting = False
        # æ¨ç†èŠ‚æµä¸åˆ†è¾¨ç‡
        self.infer_interval_ms = 150  # æœ€å°æ¨ç†é—´éš”ï¼Œé˜²æ­¢é¢‘ç¹å¡é¡¿
        self.last_infer_ts = 0
        self.infer_size = 512  # CPU é»˜è®¤ä½¿ç”¨è¾ƒä½åˆ†è¾¨ç‡ï¼ŒGPU ä¼šåœ¨åŠ è½½åè‡ªåŠ¨æå‡
        self._infer_busy = False
        # æŒ‰å¸§æ£€æµ‹ï¼ˆç¡®ä¿è¿ç»­å¤šå¸§åŠ¨æ€æ£€æµ‹ï¼‰
        self.frame_skip = 2  # CPU é»˜è®¤æ¯2å¸§æ£€æµ‹ä¸€æ¬¡ï¼›GPUä¸‹ä¼šæ”¹ä¸ºæ¯å¸§
        self.video_frame_index = 0
        self.camera_frame_index = 0
        # åå°æ¨ç†çº¿ç¨‹
        self.infer_thread = None
        # æœ€è¿‘ä¸€æ¬¡å åŠ åçš„ç»“æœå¸§ï¼ˆç”¨äºæŒç»­æ˜¾ç¤ºï¼Œé¿å…é—ªçƒï¼‰
        self._last_annotated_qimage = None
        # UI åˆ·æ–°èŠ‚æµï¼ˆæŒ‰ç›®æ ‡å¸§ç‡é™åˆ¶æ ‡ç­¾åˆ·æ–°é¢‘ç‡ï¼‰
        self._last_ui_update_ms = 0
        self.ui_min_update_interval_ms = 1000 // 30

        # åŠ è½½YOLOæ¨¡å‹ï¼ˆv5/v8ï¼‰
        # å¯åŠ¨æ—¶å…ˆåˆ·æ–°ä¸€æ¬¡æœ¬åœ°æƒé‡åˆ—è¡¨ï¼Œé¿å…å¼¹çª—
        try:
            self.refresh_local_weights()
        except Exception:
            pass
        # é»˜è®¤ä¸è‡ªåŠ¨åŠ è½½æ¨¡å‹ï¼Œç­‰å¾…ç”¨æˆ·é€‰æ‹©å…·ä½“æƒé‡
        self.model = None
        self.model_type = 'v5'
        # é€šè¿‡æŒ‰é’®é€‰æ‹©çš„æƒé‡è·¯å¾„
        self.selected_weight_path = None

        # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
        self.recent_detections = []  # å­˜å‚¨æœ€è¿‘æ£€æµ‹ç»“æœ
        self.reset_stats()
        # åˆå§‹ç¦ç”¨å¼€å§‹æ£€æµ‹ï¼Œå¾…æ¨¡å‹æˆåŠŸåŠ è½½åå¯ç”¨
        try:
            if hasattr(self, 'btn_play'):
                self.btn_play.setEnabled(False)
        except Exception:
            pass

    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.update_detection_info(0, 0, 0, 0, {})

    def _get_base_dir(self):
        """è·å–åº”ç”¨çš„åŸºç¡€ç›®å½•ï¼šå¼€å‘ç¯å¢ƒè¿”å›é¡¹ç›®ä¸Šçº§ç›®å½•ï¼Œæ‰“åŒ…ç¯å¢ƒè¿”å›å¯æ‰§è¡Œæ–‡ä»¶ç›®å½•"""
        try:
            if getattr(sys, 'frozen', False):
                return os.path.dirname(sys.executable)
            return os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))
        except Exception:
            return os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))

    def update_detection_info(self, targets, inference_time, fps, progress, class_counts, status="å¾…æœºä¸­"):
        """ä¸å†æ„å»ºå›ºå®šä¿¡æ¯æ–‡æœ¬ï¼Œä¿æŒä¸ç”¨æˆ·è¦æ±‚ä¸€è‡´ï¼Œç”±æ¨ç†çº¿ç¨‹è¿½åŠ æ—¥å¿—"""
        return

    def get_recent_detections(self):
        """è·å–æœ€è¿‘æ£€æµ‹ç»“æœ"""
        if hasattr(self, 'recent_detections') and self.recent_detections:
            return "\n   ".join(self.recent_detections[-3:])  # æ˜¾ç¤ºæœ€è¿‘3æ¡
        return "æš‚æ— æ£€æµ‹æ•°æ®"

    def get_system_tip(self, status):
        """è·å–ç³»ç»Ÿæç¤º"""
        tips = {
            "å¾…æœºä¸­": "è¯·é€‰æ‹©è¾“å…¥æºå¼€å§‹æ£€æµ‹",
            "æ£€æµ‹ä¸­": "æ­£åœ¨å¤„ç†è¾“å…¥æ•°æ®ï¼Œè¯·ç¨å€™...",
            "å®Œæˆ": "æ£€æµ‹å®Œæˆï¼Œå¯æŸ¥çœ‹ç»“æœ",
            "é”™è¯¯": "æ£€æµ‹è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œè¯·æ£€æŸ¥è¾“å…¥"
        }
        return tips.get(status, "ç³»ç»Ÿè¿è¡Œæ­£å¸¸")

    def connect_signals(self):
        """è¿æ¥æ‰€æœ‰ä¿¡å·å’Œæ§½å‡½æ•°"""
        # å‚æ•°æ§åˆ¶ä¿¡å· - ç°åœ¨é€šè¿‡æŒ‰é’®å¼¹çª—å¤„ç†ï¼Œä¸éœ€è¦ç›´æ¥è¿æ¥
        # self.conf_spin.valueChanged.connect(self.on_conf_changed)
        # self.conf_slider.valueChanged.connect(self.on_conf_slider_changed)
        # self.iou_spin.valueChanged.connect(self.on_iou_changed)
        # self.iou_slider.valueChanged.connect(self.on_iou_slider_changed)

        # è¾“å…¥æºé€‰æ‹©ä¿¡å·
        self.btn_image.clicked.connect(self.on_image_clicked)
        self.btn_video.clicked.connect(self.on_video_clicked)
        self.btn_camera.clicked.connect(self.on_camera_clicked)

        # æ¨¡å‹é€‰æ‹©ä¿¡å·
        try:
            if hasattr(self, 'btn_select_weight'):
                self.btn_select_weight.clicked.connect(self.on_select_weight_clicked)
        except Exception:
            pass

        # æ§åˆ¶ä¿¡å·
        self.btn_play.clicked.connect(self.on_play_clicked)
        self.btn_stop.clicked.connect(self.on_stop_clicked)

    # å‚æ•°è°ƒæ•´å›è°ƒå‡½æ•°
    def on_conf_changed(self, value):
        """Confidenceå€¼æ”¹å˜"""
        self.conf_slider.setValue(int(value * 100))
        self.console.append(f"ğŸ¯ Confidenceé˜ˆå€¼è°ƒæ•´ä¸º: {value:.2f}")

    def on_conf_slider_changed(self, value):
        """Confidenceæ»‘å—æ”¹å˜"""
        self.conf_spin.setValue(value / 100.0)

    def on_iou_changed(self, value):
        """IOUå€¼æ”¹å˜"""
        self.iou_slider.setValue(int(value * 100))
        self.console.append(f"ğŸ“ IOUé˜ˆå€¼è°ƒæ•´ä¸º: {value:.2f}")

    def on_iou_slider_changed(self, value):
        """IOUæ»‘å—æ”¹å˜"""
        self.iou_spin.setValue(value / 100.0)

    def load_model(self):
        """åŠ è½½YOLOæ¨¡å‹ï¼ˆv5 æˆ– v8ï¼Œä¼˜å…ˆæœ¬åœ°ä»“åº“ï¼‰ï¼Œæ”¯æŒè‡ªå®šä¹‰æƒé‡"""
        # æ¨æµ‹æœ¬åœ°ä»“åº“æ ¹ï¼šå¼€å‘ç¯å¢ƒ=é¡¹ç›®ä¸Šçº§ï¼Œæ‰“åŒ…ç¯å¢ƒ=exeç›®å½•
        base_dir = self._get_base_dir()
        # v5 ä»“åº“å€™é€‰ï¼šä¼˜å…ˆ F:/.../YOLOv5 ç›®å½•ï¼Œå…¶æ¬¡ YOLOv5/yolov5-masterï¼Œå†æ¬¡ yolo v5 åŒçº§
        v5_repo_candidates = [
            os.path.join(base_dir, 'YOLOv5'),
            os.path.join(base_dir, 'YOLOv5', 'yolov5-master'),
            os.path.join(base_dir, 'yolov5'),
            # æ‰“åŒ…ç¯å¢ƒå¸¸è§å†…ç½®è·¯å¾„
            os.path.join(base_dir, '_internal', 'YOLOv5', 'yolov5-master'),
            os.path.join(base_dir, '_internal', 'YOLOv5'),
        ]
        # ä»…å½“ç›®å½•å†…å« hubconf.py æ‰è§†ä¸ºæœ‰æ•ˆ YOLOv5 æœ¬åœ°ä»“åº“
        yolo_repo = next((p for p in v5_repo_candidates if os.path.isfile(os.path.join(p, 'hubconf.py'))), None)
        # é¿å… torch.hub ç¼“å­˜å¯¼è‡´åŠ è½½æ—§ä»£ç ï¼Œå¼ºåˆ¶æœ¬åœ°ä¼˜å…ˆæ—¶å¯å…³é—­ç¼“å­˜
        try:
            torch.hub.set_dir(os.path.join(base_dir, '.torchhub'))
        except Exception:
            pass
        if yolo_repo is None:
            self.console.append("âš ï¸ æœªæ‰¾åˆ°æœ¬åœ°YOLOv5ä»“åº“ï¼Œå°†å°è¯•è”ç½‘åŠ è½½ï¼ˆå¯èƒ½è¾ƒæ…¢ï¼‰")

        # ä½¿ç”¨æŒ‰é’®é€‰æ‹©çš„æƒé‡è·¯å¾„
        weights_path = getattr(self, 'selected_weight_path', None)
        sp = (weights_path or '').strip()
        sp_lower = os.path.basename(sp).lower()
        path_lower = (sp or '').lower()
        # å…ˆåŸºäºè·¯å¾„ååšåå¥½åˆ¤æ–­ï¼›è‹¥ä¸ç¡®å®šåˆ™è‡ªåŠ¨å°è¯•
        prefer_v8 = ('yolov8' in path_lower) or ('v8' in sp_lower)
        prefer_v5 = ('yolov5' in path_lower) or ('v5' in sp_lower)

        try:
            self.console.append(f"ğŸ§© Torch ç‰ˆæœ¬: {getattr(torch, '__version__', 'unknown')}")
            self.console.append(f"ğŸ“¦ æƒé‡è·¯å¾„: {weights_path if (weights_path and os.path.isfile(weights_path)) else 'æœªé€‰æ‹©'}")

            if not weights_path or not os.path.isfile(weights_path):
                self.console.append("â„¹ï¸ æœªé€‰æ‹©æƒé‡ï¼Œè·³è¿‡æ¨¡å‹åŠ è½½")
                if hasattr(self, 'lbl_model_status'):
                    self.lbl_model_status.setText("âŒ æ¨¡å‹æœªåŠ è½½")
                if hasattr(self, 'btn_play'):
                    self.btn_play.setEnabled(False)
                return None

            model = None

            # å®šä¹‰å››ç§åŠ è½½å™¨ï¼ˆv1/v3 å…ˆæŒ‰ v8 å°è¯•ï¼Œå¤±è´¥å†æŒ‰ v5ï¼‰
            def _try_load_v8(p):
                from ultralytics import YOLO
                return YOLO(p)

            def _try_load_v5(p):
                if yolo_repo and os.path.isdir(yolo_repo):
                    try:
                        return torch.hub.load(yolo_repo, 'custom', path=p, source='local', trust_repo=True)
                    except TypeError:
                        return torch.hub.load(yolo_repo, 'custom', path=p, source='local')
                else:
                    try:
                        return torch.hub.load('ultralytics/yolov5', 'custom', path=p, trust_repo=True)
                    except TypeError:
                        return torch.hub.load('ultralytics/yolov5', 'custom', path=p)

            # å–æ¶ˆè‡ªåŠ¨è¯†åˆ«ï¼šé»˜è®¤å¼ºåˆ¶æŒ‰ YOLOv5 åŠ è½½ï¼Œå¦‚éœ€ YOLOv8 å¯æ‰‹åŠ¨è®¾ç½® forced_model_type='v8'
            forced = getattr(self, 'forced_model_type', None)
            if forced == 'v8':
                model = _try_load_v8(weights_path)
                self.model_type = 'v8'
                self.console.append(f"âœ… å·²æŒ‰ YOLOv8 åŠ è½½: {os.path.basename(weights_path)}")
            else:
                model = _try_load_v5(weights_path)
                self.model_type = 'v5'
                self.console.append(f"âœ… å·²æŒ‰ YOLOv5 åŠ è½½: {os.path.basename(weights_path)}")

            # è®¾ç½®é˜ˆå€¼ï¼ˆä¸UIä¿æŒä¸€è‡´ï¼‰ï¼Œå…¼å®¹ä¸åŒç‰ˆæœ¬API
            try:
                if self.model_type == 'v8':
                    # YOLOv8 ä½¿ç”¨æ¨ç†å‚æ•°æ§åˆ¶ï¼Œæ— éœ€åœ¨æ¨¡å‹ä¸Šè®¾ç½®
                    pass
                else:
                    if hasattr(self, 'confidence_value') and hasattr(model, 'conf'):
                        model.conf = float(self.confidence_value)
                    if hasattr(self, 'iou_value') and hasattr(model, 'iou'):
                        model.iou = float(self.iou_value)
            except Exception:
                pass

            # æ¨ç†è®¾å¤‡
            try:
                device = 'cuda' if torch.cuda.is_available() else 'cpu'
                if self.model_type == 'v5':
                    model.to(device)
                else:
                    try:
                        model.to(device)
                    except Exception:
                        pass
                self.console.append(f"ğŸ–¥ï¸ æ¨ç†è®¾å¤‡: {device}")
                # GPU ç”¨é«˜åˆ†è¾¨ç‡ï¼ŒCPU ç”¨è¾ƒä½åˆ†è¾¨ç‡
                self.infer_size = 640 if device == 'cuda' else 512
                self.frame_skip = 1 if device == 'cuda' else 2
            except Exception:
                pass

            # æ¨¡å‹é¢„çƒ­ï¼šé¿å…é¦–å¸§æ¨ç†å¡é¡¿
            try:
                import numpy as _np
                dummy = _np.zeros((self.infer_size, self.infer_size, 3), dtype=_np.uint8)
                with torch.no_grad():
                    if self.model_type == 'v8':
                        _ = model(dummy, imgsz=int(self.infer_size), conf=float(getattr(self, 'confidence_value', 0.5)), iou=float(getattr(self, 'iou_value', 0.45)))
                    else:
                        _ = model(dummy, size=int(self.infer_size))
            except Exception:
                pass

            # æ›´æ–°æ¨¡å‹çŠ¶æ€æ ‡ç­¾
            if hasattr(self, 'lbl_model_status'):
                label_type = 'v8' if getattr(self, 'model_type', 'v5') == 'v8' else 'v5'
                self.lbl_model_status.setText(f"ğŸ“‹ æ¨¡å‹å°±ç»ª (YOLO{label_type.upper()})")

            return model
        except Exception as e:
            self.console.append(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            return None

    def detect_image(self, image_path):
        """ä½¿ç”¨YOLOv5/YOLOv8æ£€æµ‹å›¾åƒå¹¶æ˜¾ç¤ºæ ‡æ³¨ç»“æœï¼ˆæ”¯æŒä¸­æ–‡è·¯å¾„ï¼‰"""
        try:
            if self.model is None:
                self.console.append("âš ï¸ æœªåŠ è½½æ¨¡å‹ï¼Œæ— æ³•è¿›è¡Œæ£€æµ‹")
                return

            # è¯»å–åŸå§‹å›¾åƒï¼ˆBGRï¼‰ï¼Œæ”¯æŒä¸­æ–‡è·¯å¾„
            bgr = cv2.imdecode(np.fromfile(image_path, dtype=np.uint8), cv2.IMREAD_COLOR)
            if bgr is None:
                self.console.append(f"âŒ æ— æ³•è¯»å–å›¾åƒæ–‡ä»¶: {image_path}")
                return

            start_ts = time.time()
            # è®¾ç½®é˜ˆå€¼çƒ­æ›´æ–°ï¼ˆå¯¹äºv5è®¾ç½®åˆ°æ¨¡å‹ï¼Œå¯¹äºv8ä½¿ç”¨æ¨ç†å‚æ•°ï¼‰
            try:
                if getattr(self, 'model_type', 'v5') == 'v5':
                    if hasattr(self, 'confidence_value') and hasattr(self.model, 'conf'):
                        self.model.conf = float(self.confidence_value)
                    if hasattr(self, 'iou_value') and hasattr(self.model, 'iou'):
                        self.model.iou = float(self.iou_value)
            except Exception:
                pass

            # æ¨ç†ï¼ˆèŠ‚æµä¸éœ€è¦ï¼Œå› ä¸ºå•æ¬¡å›¾åƒæ£€æµ‹ï¼‰
            with torch.no_grad():
                if getattr(self, 'model_type', 'v5') == 'v8':
                    results = self.model(bgr, imgsz=int(self.infer_size), conf=float(getattr(self, 'confidence_value', 0.5)), iou=float(getattr(self, 'iou_value', 0.45)))
                    rendered = results[0].plot()
                else:
                    results = self.model(bgr, size=int(self.infer_size))
                    rendered = results.render()[0]
            inference_ms = int((time.time() - start_ts) * 1000)

            # æ˜¾ç¤ºåˆ°UIï¼ˆè½¬RGBå†æ˜¾ç¤ºï¼‰
            rgb = cv2.cvtColor(rendered, cv2.COLOR_BGR2RGB)
            h, w, ch = rgb.shape
            qt_img = QImage(rgb.data, w, h, ch * w, QImage.Format_RGB888)
            pixmap = QPixmap.fromImage(qt_img)
            scaled_pixmap = pixmap.scaled(self.lbl_video.size(), Qt.KeepAspectRatio, Qt.SmoothTransformation)
            self.lbl_video.setPixmap(scaled_pixmap)

            # è§£æç»“æœå¹¶æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            targets, class_counts, recent_text = self._parse_yolo_results(results)
            self.recent_detections.append(recent_text)
            self.update_detection_info(targets, inference_ms, 0, 100, class_counts, status="å®Œæˆ")

            # è‡ªåŠ¨ä¿å­˜ç»“æœ
            if hasattr(self, 'cb_save') and self.cb_save.isChecked():
                save_path = os.path.splitext(image_path)[0] + "_det.jpg"
                cv2.imencode('.jpg', rendered)[1].tofile(save_path)
                self.console.append(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {os.path.basename(save_path)}")

            self.console.append("âœ… å›¾åƒæ£€æµ‹å®Œæˆ")

        except Exception as e:
            self.console.append(f"âŒ å›¾åƒæ£€æµ‹å¤±è´¥: {str(e)}")

    def _parse_yolo_results(self, results):
        """ä»YOLO(v5/v8)ç»“æœä¸­ç»Ÿè®¡ç›®æ ‡æ•°ä¸ç±»åˆ«åˆ†å¸ƒï¼Œè¿”å›(æ€»æ•°, ç±»åˆ«ç»Ÿè®¡, æœ€è¿‘ç»“æœæ–‡æœ¬)"""
        try:
            if getattr(self, 'model_type', 'v5') == 'v8':
                res0 = results[0] if isinstance(results, (list, tuple)) else results
                boxes = getattr(res0, 'boxes', None)
                names = getattr(res0, 'names', getattr(self.model, 'names', {}))
                if boxes is not None and hasattr(boxes, 'cls') and boxes.cls is not None:
                    total = int(boxes.cls.shape[0])
                else:
                    total = 0
            else:
                # v5
                pred = results.xyxy[0]  # tensor: [N, 6]
                total = int(pred.shape[0]) if pred is not None else 0

            name_map = {
                'car': 'è½¦è¾†',
                'person': 'è¡Œäºº',
                'traffic light': 'äº¤é€šç¯',
                'stop sign': 'æ ‡å¿—ç‰Œ',
                'bus': 'å…¬äº¤è½¦',
                'truck': 'å¡è½¦',
                'motorcycle': 'æ‘©æ‰˜è½¦',
                'bicycle': 'è‡ªè¡Œè½¦',
            }

            class_counts = {}
            recent_text = "æ— æ£€æµ‹"
            if total > 0:
                import numpy as _np
                if getattr(self, 'model_type', 'v5') == 'v8':
                    boxes = results[0].boxes
                    conf = boxes.conf.detach().cpu().numpy() if hasattr(boxes, 'conf') else _np.array([])
                    cls = boxes.cls.detach().cpu().numpy().astype(int) if hasattr(boxes, 'cls') else _np.array([], dtype=int)
                    names = getattr(results[0], 'names', getattr(self.model, 'names', {}))
                else:
                    pred = results.xyxy[0]
                    conf = pred[:, 4].detach().cpu().numpy()
                    cls = pred[:, 5].detach().cpu().numpy().astype(int)
                    names = getattr(results, 'names', getattr(self.model, 'names', {}))

                unique_cls = _np.unique(cls)
                for ci in unique_cls:
                    en = names[int(ci)] if isinstance(names, (list, dict)) else f'class{int(ci)}'
                    cn = name_map.get(en, en)
                    class_counts[cn] = int((_np.array(cls) == ci).sum())

                # å–ç½®ä¿¡åº¦æœ€é«˜çš„å‰3ä¸ª
                if conf.size > 0:
                    top_idx = _np.argsort(-conf)[:3]
                    labels = []
                    for i in top_idx:
                        en = names[int(cls[i])] if isinstance(names, (list, dict)) else f'class{int(cls[i])}'
                        cn = name_map.get(en, en)
                        labels.append(f"{cn} {conf[i]:.2f}")
                    if labels:
                        recent_text = ", ".join(labels)

            return total, class_counts, recent_text
        except Exception:
            return 0, {}, "è§£æå¤±è´¥"

    # è¦†ç›–æ‘„åƒå¤´å¸§æ˜¾ç¤ºä»¥åŠ å…¥å®æ—¶æ£€æµ‹ï¼ˆå¯ç”±å¼€å§‹/åœæ­¢æŒ‰é’®æ§åˆ¶ï¼‰
    def show_real_video(self, h, w, c, data):
        try:
            frame_bgr = np.frombuffer(data, dtype=np.uint8).reshape((h, w, c))
            # å°†å¸§é€å…¥åå°çº¿ç¨‹è¿›è¡Œæ¨ç†
            if self.detecting and self.model is not None and self.infer_thread is not None:
                self.infer_thread.submit_frame(frame_bgr)

            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)
            raw_qimage = QImage(frame_rgb.data, w, h, c * w, QImage.Format_RGB888)

            # æ£€æµ‹ä¸­ä¼˜å…ˆæ˜¾ç¤ºæœ€è¿‘çš„å åŠ ç»“æœï¼Œé¿å…åŸå§‹å¸§è¦†ç›–é€ æˆâ€œé—ªçƒâ€ï¼›å¹¶é™åˆ¶åˆ·æ–°é¢‘ç‡
            now_ms = int(time.time() * 1000)
            if now_ms - self._last_ui_update_ms >= self.ui_min_update_interval_ms:
                if self.detecting and self._last_annotated_qimage is not None:
                    pixmap = QPixmap.fromImage(self._last_annotated_qimage)
                else:
                    pixmap = QPixmap.fromImage(raw_qimage)
                scaled_pixmap = pixmap.scaled(self.lbl_video.size(), Qt.KeepAspectRatio, Qt.FastTransformation)
                self.lbl_video.setPixmap(scaled_pixmap)
                self._last_ui_update_ms = now_ms
        except Exception as e:
            print(f"æ˜¾ç¤ºæ‘„åƒå¤´ç”»é¢é”™è¯¯: {e}")

    def display_video_frame(self, qt_image):
        """è§†é¢‘æ’­æ”¾å¸§å›è°ƒï¼šåœ¨å®æ—¶æ£€æµ‹å¼€å¯æ—¶æ‰§è¡ŒYOLOv5æ¨ç†å¹¶å åŠ ç»“æœ"""
        try:
            w = qt_image.width()
            h = qt_image.height()
            ch = 3

            # å°†å¸§é€å…¥åå°çº¿ç¨‹è¿›è¡Œæ¨ç†
            if self.detecting and self.model is not None and self.infer_thread is not None:
                bits = qt_image.bits()
                bits.setsize(h * w * ch)
                rgb = np.frombuffer(bits, dtype=np.uint8).reshape((h, w, ch))
                bgr = cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)
                self.infer_thread.submit_frame(bgr)

            # æ£€æµ‹ä¸­ä¼˜å…ˆæ˜¾ç¤ºæœ€è¿‘çš„å åŠ ç»“æœï¼Œé¿å…åŸå§‹å¸§è¦†ç›–é€ æˆâ€œé—ªçƒâ€ï¼›å¹¶é™åˆ¶åˆ·æ–°é¢‘ç‡
            now_ms = int(time.time() * 1000)
            if now_ms - self._last_ui_update_ms >= self.ui_min_update_interval_ms:
                if self.detecting and self._last_annotated_qimage is not None:
                    pixmap = QPixmap.fromImage(self._last_annotated_qimage)
                else:
                    pixmap = QPixmap.fromImage(qt_image)
                scaled_pixmap = pixmap.scaled(self.lbl_video.size(), Qt.KeepAspectRatio, Qt.FastTransformation)
                self.lbl_video.setPixmap(scaled_pixmap)
                self._last_ui_update_ms = now_ms
        except Exception as e:
            print(f"æ˜¾ç¤ºè§†é¢‘å¸§é”™è¯¯: {e}")

    # è¾“å…¥æºé€‰æ‹©å›è°ƒå‡½æ•°
    def on_image_clicked(self):
        """å›¾åƒæ£€æµ‹æŒ‰é’®ç‚¹å‡»"""
        file_path, _ = QFileDialog.getOpenFileName(
            self, "é€‰æ‹©å›¾åƒæ–‡ä»¶", "", "å›¾åƒæ–‡ä»¶ (*.png *.jpg *.jpeg *.bmp)"
        )
        if file_path:
            self.stop_all()  # åœæ­¢å…¶ä»–æ¨¡å¼
            self.current_image_path = file_path
            self.console.append(f"ğŸ–¼ï¸ å·²é€‰æ‹©å›¾åƒ: {os.path.basename(file_path)}")
            # æ˜¾ç¤ºå›¾åƒå¹¶è¿›è¡Œæ£€æµ‹
            self.detect_image(file_path)

    def on_video_clicked(self):
        """è§†é¢‘æ£€æµ‹æŒ‰é’®ç‚¹å‡»"""
        file_path, _ = QFileDialog.getOpenFileName(
            self, "é€‰æ‹©è§†é¢‘æ–‡ä»¶", "", "è§†é¢‘æ–‡ä»¶ (*.mp4 *.avi *.mov *.mkv)"
        )
        if file_path:
            self.stop_all()  # åœæ­¢å…¶ä»–æ¨¡å¼
            self.console.append(f"ğŸ¬ å·²é€‰æ‹©è§†é¢‘: {os.path.basename(file_path)}")
            self.start_video_playback(file_path)
            # å¼€å§‹åå°æ¨ç†çº¿ç¨‹
            self._start_infer_thread()

    def on_camera_clicked(self):
        """æ‘„åƒå¤´æŒ‰é’®ç‚¹å‡»"""
        if not self.camera_running:
            # åœæ­¢å…¶ä»–æ¨¡å¼
            self.stop_video_playback()

            # å°è¯•å¯åŠ¨çœŸå®æ‘„åƒå¤´
            if self.start_camera():
                self.console.append("ğŸ“¹ å¯åŠ¨çœŸå®æ‘„åƒå¤´æ£€æµ‹")
                self.btn_camera.setText("ğŸ“¹ åœæ­¢æ‘„åƒå¤´")
                self.btn_camera.setStyleSheet("""
                    QPushButton {
                        background: qlineargradient(x1: 0, y1: 0, x2: 0, y2: 1,
                            stop: 0 #e74c3c, stop: 1 #c0392b);
                        font-size: 13px;
                    }
                """)
                # é‡ç½®ç»Ÿè®¡ä¿¡æ¯
                self.reset_stats()
            else:
                self.console.append("âŒ æ— æ³•å¯åŠ¨æ‘„åƒå¤´")
        else:
            # åœæ­¢æ‘„åƒå¤´
            self.stop_camera()
            self.btn_camera.setText("ğŸ“¹ å®æ—¶æ‘„åƒå¤´")
            self.btn_camera.setStyleSheet("""
                QPushButton {
                    background: qlineargradient(x1: 0, y1: 0, x2: 0, y2: 1,
                        stop: 0 #8e44ad, stop: 1 #6c3483);
                    font-size: 13px;
                }
            """)

    # æ§åˆ¶å›è°ƒå‡½æ•°
    def on_play_clicked(self):
        """å¼€å§‹æ£€æµ‹æŒ‰é’®ç‚¹å‡»"""
        # å¼€å¯æ£€æµ‹æ¨¡å¼ï¼ˆç”¨äºæ‘„åƒå¤´/è§†é¢‘ï¼‰
        self.detecting = True
        self._start_infer_thread()
        # å¦‚å‹¾é€‰ä¿å­˜ç»“æœï¼Œè¯¢é—®å›¾ç‰‡åºåˆ—ä¿å­˜æ–‡ä»¶å¤¹
        if hasattr(self, 'cb_save') and self.cb_save.isChecked():
            if (self.camera_running or (self.video_thread and self.video_thread.isRunning())):
                self._prompt_save_image_dir()
        if self.camera_running:
            self.console.append("â–¶ï¸ å·²å¯ç”¨æ‘„åƒå¤´å®æ—¶æ£€æµ‹")
        elif self.video_thread and self.video_thread.isRunning():
            self.console.append("â–¶ï¸ å·²å¯ç”¨è§†é¢‘å®æ—¶æ£€æµ‹")
        elif self.current_image_path:
            self.console.append("â–¶ï¸ å¼€å§‹å›¾åƒæ£€æµ‹")
            self.detect_image(self.current_image_path)
        else:
            self.console.append("âš ï¸ è¯·å…ˆé€‰æ‹©è¾“å…¥æº")

    def on_stop_clicked(self):
        """åœæ­¢æ£€æµ‹æŒ‰é’®ç‚¹å‡»"""
        # å…³é—­å®æ—¶æ£€æµ‹
        self.detecting = False
        self.stop_all()
        self._stop_infer_thread()
        self.console.append("â¹ï¸ åœæ­¢æ‰€æœ‰æ£€æµ‹")
        # é‡ç½®ç»Ÿè®¡ä¿¡æ¯
        self.reset_stats()
        # åœæ­¢åä¸å†ä¿ç•™å†å²è¾“å‡ºï¼ˆæ ¹æ®éœ€è¦å¯æ³¨é‡Šæ‰ï¼‰
        self.detection_info.setPlainText("")
        # æ¸…ç©ºå åŠ å¸§ç¼“å­˜
        self._last_annotated_qimage = None
        # æ¸…ç†ä¿å­˜çŠ¶æ€
        self._clear_save_state()

    # å·²ç§»é™¤æ¨¡å‹ç³»åˆ—é€‰æ‹©

    # æ—§çš„ä¸‹æ‹‰åˆ‡æ¢é€»è¾‘å·²åºŸå¼ƒ

    def refresh_local_weights(self):
        """å…¼å®¹æ—§æ¥å£ï¼šä¸å†åˆ·æ–°ä¸‹æ‹‰ï¼›æ”¹ä¸ºæŒ‰é’®é€‰æ‹©æ—¶æ‰“å¼€ç›®å½•æµè§ˆã€‚"""
        return

    def closeEvent(self, event):
        """çª—å£å…³é—­äº‹ä»¶"""
        self.stop_all()
        self._stop_infer_thread()
        self._last_annotated_qimage = None
        event.accept()

    def _start_infer_thread(self):
        """å¯åŠ¨æˆ–æ›´æ–°åå°æ¨ç†çº¿ç¨‹å‚æ•°"""
        if self.model is None:
            return
        if self.infer_thread is None:
            # æ ¹æ®æƒé‡æ–‡ä»¶åæ¨æ–­çº¿ç¨‹çš„æ¨¡å‹ç±»å‹
            model_type = getattr(self, 'model_type', 'v5')
            self.infer_thread = InferenceThread(self.model, self.infer_size, self.frame_skip, self.infer_interval_ms, model_type=model_type, conf=getattr(self, 'confidence_value', 0.5), iou=getattr(self, 'iou_value', 0.45))
            # ä½¿ç”¨QueuedConnectionç¡®ä¿è·¨çº¿ç¨‹UIä¿¡å·å®‰å…¨
            try:
                from PyQt5.QtCore import Qt as _Qt
                self.infer_thread.frame_ready.connect(self._on_infer_frame_ready, type=_Qt.QueuedConnection)
                self.infer_thread.meta_ready.connect(self._on_infer_meta_ready, type=_Qt.QueuedConnection)
            except Exception:
                self.infer_thread.frame_ready.connect(self._on_infer_frame_ready)
                self.infer_thread.meta_ready.connect(self._on_infer_meta_ready)
            try:
                self.infer_thread.setObjectName("InferenceThread")
            except Exception:
                pass
            self.infer_thread.start()
        else:
            model_type = getattr(self, 'model_type', 'v5')
            self.infer_thread.update_params(self.infer_size, self.frame_skip, self.infer_interval_ms, conf=getattr(self, 'confidence_value', 0.5), iou=getattr(self, 'iou_value', 0.45), model_type=model_type)

    def _stop_infer_thread(self):
        if self.infer_thread is not None:
            try:
                self.infer_thread.stop()
                self.infer_thread.wait(2000)
            except Exception:
                pass
            self.infer_thread = None
    
    def _clear_save_state(self):
        try:
            if hasattr(self, 'save_image_dir'):
                del self.save_image_dir
            if hasattr(self, 'save_image_index'):
                del self.save_image_index
        except Exception:
            pass

    def _on_infer_frame_ready(self, qt_image: QImage):
        """æ”¶åˆ°åå°æ¨ç†ç»“æœå¸§ï¼Œéé˜»å¡æ›´æ–°UI"""
        try:
            # ç¼“å­˜æœ€æ–°çš„å åŠ ç»“æœï¼Œä¾›åŸå§‹å¸§åˆ°è¾¾æ—¶ä¹Ÿä¼˜å…ˆæ˜¾ç¤º
            self._last_annotated_qimage = qt_image
            now_ms = int(time.time() * 1000)
            if now_ms - self._last_ui_update_ms >= self.ui_min_update_interval_ms:
                pixmap = QPixmap.fromImage(qt_image)
                scaled_pixmap = pixmap.scaled(self.lbl_video.size(), Qt.KeepAspectRatio, Qt.FastTransformation)
                self.lbl_video.setPixmap(scaled_pixmap)
                self._last_ui_update_ms = now_ms

            # å†™å…¥å›¾ç‰‡åºåˆ—
            if getattr(self, 'save_image_dir', None):
                try:
                    w = qt_image.width()
                    h = qt_image.height()
                    ch = 3
                    tmp_img = qt_image.copy()
                    bits = tmp_img.bits()
                    bits.setsize(h * w * ch)
                    rgb = np.frombuffer(bits, dtype=np.uint8).reshape((h, w, ch))
                    bgr = cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)
                    name = f"frame_{time.strftime('%Y%m%d_%H%M%S')}_{getattr(self, 'save_image_index', 0):06d}.jpg"
                    cv2.imwrite(os.path.join(self.save_image_dir, name), bgr)
                    self.save_image_index = getattr(self, 'save_image_index', 0) + 1
                except Exception:
                    pass
        except Exception:
            pass

    def _prompt_save_image_dir(self):
        try:
            default_dir = os.path.join(os.path.expanduser("~"), "Detections")
            if not os.path.isdir(default_dir):
                try:
                    os.makedirs(default_dir, exist_ok=True)
                except Exception:
                    default_dir = os.path.expanduser("~")

            folder = QFileDialog.getExistingDirectory(self, "é€‰æ‹©ä¿å­˜å›¾ç‰‡åºåˆ—çš„æ–‡ä»¶å¤¹", default_dir)
            if folder:
                self.save_image_dir = folder
                self.save_image_index = 0
                self.console.append(f"ğŸ’¾ ä¿å­˜å›¾ç‰‡åºåˆ—åˆ°: {folder}")
            else:
                self._clear_save_state()
        except Exception as e:
            self.console.append(f"âŒ é€‰æ‹©ä¿å­˜è·¯å¾„å¤±è´¥: {str(e)}")
            self._clear_save_state()

    def on_select_weight_clicked(self):
        """ç‚¹å‡»â€˜é€‰æ‹©æƒé‡â€™ï¼šå…ˆé€‰ YOLO ç±»å‹(v3/v5/v8)ï¼Œå†åœ¨å›ºå®šç›®å½•ä¸­é€‰ .pt æ–‡ä»¶ã€‚"""
        try:
            types = ["YOLOv3", "YOLOv5", "YOLOv8"]
            model_type, ok = QInputDialog.getItem(self, "é€‰æ‹©æ¨¡å‹ç±»å‹", "æ¨¡å‹ç±»å‹:", types, 1, False)
            if not ok or not model_type:
                self.console.append("â„¹ï¸ å·²å–æ¶ˆé€‰æ‹©æ¨¡å‹ç±»å‹")
                return

            fixed_dirs = {
                'YOLOv3': r"F:\\desktop\\SEU\\å“å·¥\\YOLOv3",
                'YOLOv5': r"F:\\desktop\\SEU\\å“å·¥\\YOLOv5",
                'YOLOv8': r"F:\\desktop\\SEU\\å“å·¥\\YOLOv8",
            }
            start_dir = fixed_dirs.get(model_type, r"F:\\desktop\\SEU\\å“å·¥")
            if not os.path.isdir(start_dir):
                self.console.append(f"âš ï¸ ç›®å½•ä¸å­˜åœ¨: {start_dir}ï¼Œå›é€€åˆ°ç”¨æˆ·ç›®å½•")
                start_dir = os.path.expanduser("~")

            file_path, _ = QFileDialog.getOpenFileName(self, f"é€‰æ‹© {model_type} æƒé‡(.pt)", start_dir, "PyTorch æƒé‡ (*.pt)")
            if not file_path or not os.path.isfile(file_path):
                self.console.append("â„¹ï¸ å·²å–æ¶ˆé€‰æ‹©æƒé‡")
                return

            self.selected_weight_path = file_path
            self.forced_model_type = 'v8' if model_type == 'YOLOv8' else 'v5'
            if hasattr(self, 'lbl_weight_path'):
                self.lbl_weight_path.setText(file_path)

            self.console.append("ğŸ” åŠ è½½æ‰€é€‰æƒé‡...")
            self.model = self.load_model()
            if self.model is None:
                self.console.append("âŒ æ¨¡å‹åŠ è½½å¤±è´¥")
                return

            if self.infer_thread is not None:
                model_type_flag = getattr(self, 'model_type', 'v5')
                self.infer_thread.update_params(self.infer_size, self.frame_skip, self.infer_interval_ms, conf=getattr(self, 'confidence_value', 0.5), iou=getattr(self, 'iou_value', 0.45), model_type=model_type_flag)
            self.console.append("âœ… æƒé‡åŠ è½½å®Œæˆ")
            if hasattr(self, 'btn_play'):
                self.btn_play.setEnabled(True)
        except Exception as e:
            self.console.append(f"âŒ é€‰æ‹©æƒé‡å¤±è´¥: {str(e)}")

    def _open_video_writer(self, w: int, h: int):
        return

    def _close_video_writer(self):
        return

    def _on_infer_meta_ready(self, text: str):
        try:
            # é€æ¡è¿½åŠ æ‰€æœ‰æ£€æµ‹è®°å½•ï¼Œä¸æ¸…ç©ºå†å²
            curr = self.detection_info.toPlainText()
            new_text = (curr + "\n" + text).strip() if curr else text
            self.detection_info.setPlainText(new_text)
            # æ»šåŠ¨åˆ°æœ«å°¾
            self.detection_info.moveCursor(self.detection_info.textCursor().End)
        except Exception:
            pass