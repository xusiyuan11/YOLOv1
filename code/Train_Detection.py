"""
Stage 2: Detection Head Training
é˜¶æ®µ2ï¼šæ£€æµ‹å¤´è®­ç»ƒ
"""
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import os
import time
from tqdm import tqdm
import numpy as np
from typing import Dict, Optional

from NetModel import create_detection_model
from YOLOLoss import YOLOLoss
from OPT import create_yolo_optimizer, create_adam_optimizer
from Utils import load_hyperparameters, save_checkpoint, load_checkpoint
from dataset import VOC_Detection_Set


class DetectionTrainer:
    """æ£€æµ‹å¤´è®­ç»ƒå™¨"""
    
    def __init__(self, 
                 hyperparameters: Dict = None,
                 save_dir: str = './checkpoints/detection'):
        """
        åˆå§‹åŒ–æ£€æµ‹è®­ç»ƒå™¨
        Args:
            hyperparameters: è¶…å‚æ•°
            save_dir: æ¨¡å‹ä¿å­˜ç›®å½•
        """
        # åŠ è½½è¶…å‚æ•°
        if hyperparameters is None:
            hyperparameters = load_hyperparameters()
        self.hyperparameters = hyperparameters
        self.save_dir = save_dir
        
        # è®¾ç½®è®¾å¤‡
        self.device = torch.device(hyperparameters.get('device', 'cuda' if torch.cuda.is_available() else 'cpu'))
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_dir, exist_ok=True)
        
        # è®­ç»ƒç»Ÿè®¡
        self.train_losses = []
        self.val_losses = []
        self.train_maps = []
        self.val_maps = []
        
        print(f"æ£€æµ‹å¤´è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"è®¾å¤‡: {self.device}")
        print(f"ä¿å­˜ç›®å½•: {self.save_dir}")
    
    def create_datasets(self, voc_config: Dict):
        """åˆ›å»ºVOCæ£€æµ‹æ•°æ®é›†"""
        print("æ­£åœ¨åŠ è½½VOCæ£€æµ‹æ•°æ®é›†...")
        
        try:
            # è®­ç»ƒé›†
            train_dataset = VOC_Detection_Set(
                voc_data_path=voc_config['voc_data_path'],
                input_size=self.hyperparameters.get('input_size', 448),
                grid_size=self.hyperparameters.get('grid_size', 7),
                is_train=True
            )
            
            # éªŒè¯é›†
            val_dataset = VOC_Detection_Set(
                voc_data_path=voc_config['voc_data_path'],
                input_size=self.hyperparameters.get('input_size', 448),
                grid_size=self.hyperparameters.get('grid_size', 7),
                is_train=False
            )
            
            print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_dataset)}")
            print(f"éªŒè¯é›†æ ·æœ¬æ•°: {len(val_dataset)}")
            
            return train_dataset, val_dataset
            
        except Exception as e:
            print(f"VOCæ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
            print("ä½¿ç”¨è™šæ‹Ÿæ•°æ®è¿›è¡Œæ¼”ç¤º...")
            
            # åˆ›å»ºè™šæ‹Ÿæ•°æ®é›†
            from torch.utils.data import TensorDataset
            input_size = self.hyperparameters.get('input_size', 448)
            grid_size = self.hyperparameters.get('grid_size', 7)
            
            # è®­ç»ƒé›†
            train_images = torch.randn(800, 3, input_size, input_size)
            train_targets = torch.randn(800, grid_size, grid_size, 30)  # YOLOæ ¼å¼
            train_dataset = TensorDataset(train_images, train_targets)
            
            # éªŒè¯é›†
            val_images = torch.randn(200, 3, input_size, input_size)
            val_targets = torch.randn(200, grid_size, grid_size, 30)
            val_dataset = TensorDataset(val_images, val_targets)
            
            return train_dataset, val_dataset
    
    def calculate_map(self, model, val_loader):
        """è®¡ç®—mAPï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        model.eval()
        # è¿™é‡Œåº”è¯¥å®ç°å®Œæ•´çš„mAPè®¡ç®—ï¼Œæš‚æ—¶è¿”å›ä¼ªå€¼
        return np.random.uniform(0.3, 0.8)  # æ¨¡æ‹ŸmAPå€¼
    
    def train_epoch(self, model, train_loader, criterion, optimizer, epoch, freeze_backbone=False):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        model.train()
        
        # æ ¹æ®è®¾ç½®å†»ç»“æˆ–è§£å†»backbone
        if freeze_backbone:
            model.freeze_backbone()
        else:
            model.unfreeze_backbone()
        
        total_loss = 0.0
        num_batches = 0
        
        progress_bar = tqdm(train_loader, desc=f'æ£€æµ‹è®­ç»ƒ Epoch {epoch+1}')
        
        for batch_idx, (images, targets) in enumerate(progress_bar):
            images = images.to(self.device)
            targets = targets.to(self.device)
            
            # å‰å‘ä¼ æ’­
            optimizer.zero_grad()
            predictions = model(images)
            loss = criterion(predictions, targets)
            
            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            num_batches += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            avg_loss = total_loss / num_batches
            progress_bar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Avg Loss': f'{avg_loss:.4f}',
                'Backbone': 'Frozen' if freeze_backbone else 'Unfrozen'
            })
        
        avg_loss = total_loss / len(train_loader)
        return avg_loss
    
    def validate_epoch(self, model, val_loader, criterion, epoch):
        """éªŒè¯ä¸€ä¸ªepoch"""
        model.eval()
        total_loss = 0.0
        
        with torch.no_grad():
            progress_bar = tqdm(val_loader, desc=f'æ£€æµ‹éªŒè¯ Epoch {epoch+1}')
            
            for images, targets in progress_bar:
                images = images.to(self.device)
                targets = targets.to(self.device)
                
                predictions = model(images)
                loss = criterion(predictions, targets)
                
                total_loss += loss.item()
                
                progress_bar.set_postfix({
                    'Val Loss': f'{loss.item():.4f}'
                })
        
        avg_loss = total_loss / len(val_loader)
        
        # è®¡ç®—mAP
        map_score = self.calculate_map(model, val_loader)
        
        return avg_loss, map_score
    
    def train(self, 
              voc_config: Dict,
              backbone_path: str,
              epochs: int = 50,
              freeze_epochs: int = 10,
              resume_from: str = None):
        """
        è®­ç»ƒæ£€æµ‹å™¨
        Args:
            voc_config: VOCæ•°æ®é…ç½®
            backbone_path: é¢„è®­ç»ƒbackboneè·¯å¾„
            epochs: æ€»è®­ç»ƒè½®æ•°
            freeze_epochs: å†»ç»“backboneçš„è½®æ•°
            resume_from: æ¢å¤è®­ç»ƒçš„checkpointè·¯å¾„
        """
        print("="*60)
        print("é˜¶æ®µ2ï¼šæ£€æµ‹å¤´è®­ç»ƒ")
        print("="*60)
        
        # æ£€æŸ¥backboneæ–‡ä»¶
        if not os.path.exists(backbone_path):
            raise FileNotFoundError(f"é¢„è®­ç»ƒbackboneæ–‡ä»¶ä¸å­˜åœ¨: {backbone_path}")
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset, val_dataset = self.create_datasets(voc_config)
        
        train_loader = DataLoader(
            train_dataset,
            batch_size=voc_config.get('batch_size', 8),
            shuffle=True,
            num_workers=4,
            pin_memory=True
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=voc_config.get('batch_size', 8),
            shuffle=False,
            num_workers=4,
            pin_memory=True
        )
        
        # åˆ›å»ºæ£€æµ‹æ¨¡å‹
        input_size = self.hyperparameters.get('input_size', 448)
        grid_size = self.hyperparameters.get('grid_size', 7)
        use_efficient = self.hyperparameters.get('use_efficient_backbone', True)
        num_classes = voc_config.get('num_classes', 20)
        
        model = create_detection_model(
            class_num=num_classes,
            grid_size=grid_size,
            input_size=input_size,
            use_efficient_backbone=use_efficient,
            pretrained_backbone_path=backbone_path
        ).to(self.device)
        
        print(f"æ£€æµ‹æ¨¡å‹åˆ›å»ºå®Œæˆ")
        print(f"å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        print(f"ç±»åˆ«æ•°: {num_classes}")
        print(f"é¢„è®­ç»ƒbackboneå·²åŠ è½½: {backbone_path}")
        
        # åˆ›å»ºæŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        criterion = YOLOLoss(
            feature_size=grid_size,
            num_bboxes=2,
            num_classes=num_classes,
            lambda_coord=5.0,
            lambda_noobj=0.5
        )
        
        # åˆ†ç»„å‚æ•°ï¼Œä¸ºbackboneå’Œæ£€æµ‹å¤´è®¾ç½®ä¸åŒå­¦ä¹ ç‡
        backbone_params = []
        detection_params = []
        
        for name, param in model.named_parameters():
            if 'backbone' in name:
                backbone_params.append(param)
            else:
                detection_params.append(param)
        
        optimizer = torch.optim.Adam([
            {'params': backbone_params, 'lr': voc_config.get('backbone_lr', 0.0001)},
            {'params': detection_params, 'lr': voc_config.get('detection_lr', 0.001)}
        ], weight_decay=self.hyperparameters.get('weight_decay', 0.0005))
        
        # æ¢å¤è®­ç»ƒ
        start_epoch = 0
        best_map = 0.0
        
        if resume_from and os.path.exists(resume_from):
            print(f"ä» {resume_from} æ¢å¤è®­ç»ƒ...")
            checkpoint = load_checkpoint(resume_from)
            model.load_state_dict(checkpoint['model_state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            start_epoch = checkpoint['epoch']
            best_map = checkpoint.get('best_map', 0.0)
            self.train_losses = checkpoint.get('train_losses', [])
            self.val_losses = checkpoint.get('val_losses', [])
            self.train_maps = checkpoint.get('train_maps', [])
            self.val_maps = checkpoint.get('val_maps', [])
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)
        
        # è®­ç»ƒå¾ªç¯\n        print(f\"å¼€å§‹è®­ç»ƒï¼Œå…± {epochs} ä¸ªepoch\")\n        print(f\"å‰ {freeze_epochs} ä¸ªepochå°†å†»ç»“backbone\")\n        \n        for epoch in range(start_epoch, epochs):\n            # ç¡®å®šæ˜¯å¦å†»ç»“backbone\n            freeze_backbone = epoch < freeze_epochs\n            \n            # è®­ç»ƒ\n            train_loss = self.train_epoch(\n                model, train_loader, criterion, optimizer, epoch, freeze_backbone\n            )\n            \n            # éªŒè¯\n            val_loss, val_map = self.validate_epoch(model, val_loader, criterion, epoch)\n            \n            # å­¦ä¹ ç‡è°ƒåº¦\n            scheduler.step()\n            \n            # è®°å½•ç»Ÿè®¡ä¿¡æ¯\n            self.train_losses.append(train_loss)\n            self.val_losses.append(val_loss)\n            train_map = self.calculate_map(model, train_loader)  # è®­ç»ƒé›†mAP\n            self.train_maps.append(train_map)\n            self.val_maps.append(val_map)\n            \n            # æ‰“å°epochç»“æœ\n            print(f\"Epoch {epoch+1}/{epochs}:\")\n            print(f\"  è®­ç»ƒ - Loss: {train_loss:.4f}, mAP: {train_map:.4f}\")\n            print(f\"  éªŒè¯ - Loss: {val_loss:.4f}, mAP: {val_map:.4f}\")\n            print(f\"  å­¦ä¹ ç‡: {scheduler.get_last_lr()}\")\n            print(f\"  Backbone: {'å†»ç»“' if freeze_backbone else 'è®­ç»ƒ'}\")\n            \n            # ä¿å­˜æœ€ä½³æ¨¡å‹\n            if val_map > best_map:\n                best_map = val_map\n                best_model_path = os.path.join(self.save_dir, 'best_detection_model.pth')\n                save_checkpoint({\n                    'epoch': epoch + 1,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'scheduler_state_dict': scheduler.state_dict(),\n                    'best_map': best_map,\n                    'train_losses': self.train_losses,\n                    'val_losses': self.val_losses,\n                    'train_maps': self.train_maps,\n                    'val_maps': self.val_maps,\n                    'hyperparameters': self.hyperparameters\n                }, best_model_path)\n                print(f\"  æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {best_model_path}\")\n            \n            # å®šæœŸä¿å­˜checkpoint\n            if (epoch + 1) % 10 == 0:\n                checkpoint_path = os.path.join(self.save_dir, f'detection_checkpoint_epoch_{epoch+1}.pth')\n                save_checkpoint({\n                    'epoch': epoch + 1,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'scheduler_state_dict': scheduler.state_dict(),\n                    'best_map': best_map,\n                    'train_losses': self.train_losses,\n                    'val_losses': self.val_losses,\n                    'train_maps': self.train_maps,\n                    'val_maps': self.val_maps,\n                    'hyperparameters': self.hyperparameters\n                }, checkpoint_path)\n        \n        print(f\"è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯mAP: {best_map:.4f}\")\n        \n        return best_map\n\n\ndef main():\n    \"\"\"ä¸»å‡½æ•°ï¼šå•ç‹¬è¿è¡Œæ£€æµ‹è®­ç»ƒ\"\"\"\n    # åŠ è½½é…ç½®\n    hyperparameters = load_hyperparameters()\n    \n    # VOCæ•°æ®é…ç½®\n    voc_config = {\n        'voc_data_path': './data/VOC2012',\n        'batch_size': 8,\n        'backbone_lr': 0.0001,  # backboneè¾ƒå°å­¦ä¹ ç‡\n        'detection_lr': 0.001,  # æ£€æµ‹å¤´è¾ƒå¤§å­¦ä¹ ç‡\n        'num_classes': 20\n    }\n    \n    # é¢„è®­ç»ƒbackboneè·¯å¾„ï¼ˆéœ€è¦å…ˆè¿è¡Œåˆ†ç±»è®­ç»ƒï¼‰\n    backbone_path = './checkpoints/classification/trained_backbone.pth'\n    \n    if not os.path.exists(backbone_path):\n        print(f\"é”™è¯¯ï¼šæ‰¾ä¸åˆ°é¢„è®­ç»ƒbackboneæ–‡ä»¶: {backbone_path}\")\n        print(\"è¯·å…ˆè¿è¡Œ Train_Classification.py å®Œæˆåˆ†ç±»é¢„è®­ç»ƒ\")\n        return\n    \n    # åˆ›å»ºè®­ç»ƒå™¨\n    trainer = DetectionTrainer(\n        hyperparameters=hyperparameters,\n        save_dir='./checkpoints/detection'\n    )\n    \n    # å¼€å§‹è®­ç»ƒ\n    best_map = trainer.train(\n        voc_config=voc_config,\n        backbone_path=backbone_path,\n        epochs=50,\n        freeze_epochs=10,  # å‰10ä¸ªepochå†»ç»“backbone\n        resume_from=None  # å¦‚æœè¦æ¢å¤è®­ç»ƒï¼ŒæŒ‡å®šcheckpointè·¯å¾„\n    )\n    \n    print(f\"æ£€æµ‹è®­ç»ƒå®Œæˆï¼\")\n    print(f\"æœ€ä½³mAP: {best_map:.4f}\")\n\n\nif __name__ == \"__main__\":\n    main()
def main():
    """ä¸»å‡½æ•°ï¼šå•ç‹¬è¿è¡Œæ£€æµ‹è®­ç»ƒ"""
    # åŠ è½½é…ç½®
    hyperparameters = load_hyperparameters()
    
    # æ£€æŸ¥æ•°æ®è·¯å¾„å¹¶æä¾›çµæ´»çš„é…ç½®é€‰é¡¹
    possible_voc_paths = [
        '../data/VOC2012/VOCdevkit',  # æ ‡å‡†è·¯å¾„
        '../data/VOC2012',           # å¦‚æœVOCdevkitç›´æ¥åœ¨VOC2012ä¸‹
        './data/VOC2012/VOCdevkit',  # ç›¸å¯¹äºå½“å‰ç›®å½•
        './data/VOC2012',
        '../data/VOCdevkit',         # å¦‚æœVOCdevkitç›´æ¥åœ¨dataä¸‹
    ]
    
    voc_data_path = None
    for path in possible_voc_paths:
        if os.path.exists(path):
            voc_data_path = path
            print(f"âœ… æ‰¾åˆ°VOCæ•°æ®è·¯å¾„: {path}")
            break
    
    if voc_data_path is None:
        print("âŒ æœªæ‰¾åˆ°VOCæ•°æ®é›†ï¼Œè¯·æ£€æŸ¥ä»¥ä¸‹è·¯å¾„:")
        for path in possible_voc_paths:
            print(f"   - {path}")
        print("\nğŸ’¡ æç¤º: è¯·ç¡®ä¿VOCæ•°æ®é›†çš„ç›®å½•ç»“æ„å¦‚ä¸‹:")
        print("   data/VOC2012/VOCdevkit/")
        print("   â”œâ”€â”€ VOC2007/")
        print("   â”‚   â”œâ”€â”€ JPEGImages/")
        print("   â”‚   â””â”€â”€ Annotations/")
        print("   â””â”€â”€ VOC2012/")
        print("       â”œâ”€â”€ JPEGImages/")
        print("       â””â”€â”€ Annotations/")
        return
    
    # VOCæ•°æ®é…ç½®
    voc_config = {
        'voc_data_path': voc_data_path,
        'batch_size': 8,
        'backbone_lr': 0.0001,  # backboneè¾ƒå°å­¦ä¹ ç‡
        'detection_lr': 0.001,  # æ£€æµ‹å¤´è¾ƒå¤§å­¦ä¹ ç‡
        'num_classes': 20
    }
    
    # é¢„è®­ç»ƒbackboneè·¯å¾„ï¼ˆéœ€è¦å…ˆè¿è¡Œåˆ†ç±»è®­ç»ƒï¼‰
    backbone_path = './checkpoints/classification/trained_backbone.pth'
    
    if not os.path.exists(backbone_path):
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°é¢„è®­ç»ƒbackboneæ–‡ä»¶: {backbone_path}")
        print("è¯·å…ˆè¿è¡Œ Train_Classification.py å®Œæˆåˆ†ç±»é¢„è®­ç»ƒ")
        return
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = DetectionTrainer(
        hyperparameters=hyperparameters,
        save_dir='./checkpoints/detection'
    )
    
    # å¼€å§‹è®­ç»ƒ
    best_map = trainer.train(
        voc_config=voc_config,
        backbone_path=backbone_path,
        epochs=50,
        freeze_epochs=10,  # å‰10ä¸ªepochå†»ç»“backbone
        resume_from=None  # å¦‚æœè¦æ¢å¤è®­ç»ƒï¼ŒæŒ‡å®šcheckpointè·¯å¾„
    )
    
    print(f"æ£€æµ‹è®­ç»ƒå®Œæˆï¼")
    print(f"æœ€ä½³mAP: {best_map:.4f}")


if __name__ == "__main__":
    main()
